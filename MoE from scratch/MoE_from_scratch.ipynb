{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lVtp8o0zGBzx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the expert model"
      ],
      "metadata": {
        "id": "HImmXL2u1dqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Expert(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(Expert, self).__init__()\n",
        "    self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.layer1(x))\n",
        "    return torch.softmax(self.layer2(x), dim=-1)"
      ],
      "metadata": {
        "id": "a8ex0mdCGOsG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the gating model"
      ],
      "metadata": {
        "id": "5V_Co_gd1iCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gating(nn.Module):\n",
        "  def __init__(self, input_dim, num_experts, dropout_rate=0.1):\n",
        "    super(Gating, self).__init__()\n",
        "\n",
        "    self.layer1 = nn.Linear(input_dim, 128)\n",
        "    self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    self.layer2 = nn.Linear(128, 256)\n",
        "    self.leaky_relu = nn.LeakyReLU()\n",
        "    self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    self.layer3 = nn.Linear(256, 128)\n",
        "    self.leaky_relu = nn.LeakyReLU()\n",
        "    self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    self.layer4 = nn.Linear(128, num_experts)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.layer1(x))\n",
        "    x = self.dropout1(x)\n",
        "\n",
        "    x = self.layer2(x)\n",
        "    x = self.leaky_relu(x)\n",
        "    x = self.dropout2(x)\n",
        "\n",
        "    x = self.layer3(x)\n",
        "    x = self.leaky_relu(x)\n",
        "    x = self.dropout3(x)\n",
        "\n",
        "    return torch.softmax(self.layer4(x), dim=1)"
      ],
      "metadata": {
        "id": "xMEfkUqsGxpX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MoE(nn.Module):\n",
        "  def __init__(self, trained_experts):\n",
        "    super(MoE, self).__init__()\n",
        "    self.experts = nn.ModuleList(trained_experts)\n",
        "\n",
        "    # Freeze experts while MoE is training\n",
        "    for expert in self.experts:\n",
        "      for param in expert.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    num_experts = len(trained_experts)\n",
        "    # Assuming all experts have the same input dimension\n",
        "    input_dim = trained_experts[0].layer1.in_features\n",
        "    self.gating = Gating(input_dim, num_experts)\n",
        "\n",
        "  def forward(self, x):\n",
        "    weights = self.gating(x)\n",
        "\n",
        "    outputs = torch.stack([expert(x) for expert in self.experts], dim=2)\n",
        "\n",
        "    weights = weights.unsqueeze(1).expand_as(outputs)\n",
        "\n",
        "    return torch.sum(weights * outputs, dim=2)"
      ],
      "metadata": {
        "id": "z8npxpQjIUPD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate the dataset"
      ],
      "metadata": {
        "id": "__gp6oeU1ngR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the dataset\n",
        "num_samples = 5000\n",
        "input_dim = 4\n",
        "hidden_dim = 32\n",
        "\n",
        "# Generate equal numbers of labels 0, 1, and 2\n",
        "y_data = torch.cat([\n",
        "    torch.zeros(num_samples // 3),\n",
        "    torch.ones(num_samples // 3),\n",
        "    torch.full((num_samples - 2 * (num_samples // 3),), 2)  # Filling the remaining to ensure exact num_samples\n",
        "]).long()\n",
        "\n",
        "# Biasing the data based on the labels\n",
        "x_data = torch.randn(num_samples, input_dim)\n",
        "\n",
        "for i in range(num_samples):\n",
        "    if y_data[i] == 0:\n",
        "        x_data[i, 0] += 1  # Making x[0] more positive\n",
        "    elif y_data[i] == 1:\n",
        "        x_data[i, 1] -= 1  # Making x[1] more negative\n",
        "    elif y_data[i] == 2:\n",
        "        x_data[i, 0] -= 1  # Making x[0] more negative\n",
        "\n",
        "# Shuffle the data to randomize the order\n",
        "indices = torch.randperm(num_samples)\n",
        "x_data = x_data[indices]\n",
        "y_data = y_data[indices]\n",
        "\n",
        "# Verify the label distribution\n",
        "y_data.bincount()\n",
        "\n",
        "# Shuffle the data to ensure x_data and y_data remain aligned\n",
        "shuffled_indices = torch.randperm(num_samples)\n",
        "x_data = x_data[shuffled_indices]\n",
        "y_data = y_data[shuffled_indices]\n",
        "\n",
        "# Splitting data for training individual experts\n",
        "# Use the first half samples for training individual experts\n",
        "x_train_experts = x_data[:int(num_samples/2)]\n",
        "y_train_experts = y_data[:int(num_samples/2)]\n",
        "\n",
        "mask_expert1 = (y_train_experts == 0) | (y_train_experts == 1)\n",
        "mask_expert2 = (y_train_experts == 1) | (y_train_experts == 2)\n",
        "mask_expert3 = (y_train_experts == 0) | (y_train_experts == 2)\n",
        "\n",
        "# Select an almost equal number of samples for each expert\n",
        "num_samples_per_expert = \\\n",
        "min(mask_expert1.sum(), mask_expert2.sum(), mask_expert3.sum())\n",
        "\n",
        "x_expert1 = x_train_experts[mask_expert1][:num_samples_per_expert]\n",
        "y_expert1 = y_train_experts[mask_expert1][:num_samples_per_expert]\n",
        "\n",
        "x_expert2 = x_train_experts[mask_expert2][:num_samples_per_expert]\n",
        "y_expert2 = y_train_experts[mask_expert2][:num_samples_per_expert]\n",
        "\n",
        "x_expert3 = x_train_experts[mask_expert3][:num_samples_per_expert]\n",
        "y_expert3 = y_train_experts[mask_expert3][:num_samples_per_expert]\n",
        "\n",
        "# Splitting the next half samples for training MoE model and for testing\n",
        "x_remaining = x_data[int(num_samples/2)+1:]\n",
        "y_remaining = y_data[int(num_samples/2)+1:]\n",
        "\n",
        "split = int(0.8 * len(x_remaining))\n",
        "x_train_moe = x_remaining[:split]\n",
        "y_train_moe = y_remaining[:split]\n",
        "\n",
        "x_test = x_remaining[split:]\n",
        "y_test = y_remaining[split:]\n",
        "\n",
        "print(x_train_moe.shape,\"\\n\", x_test.shape,\"\\n\",\n",
        "      x_expert1.shape,\"\\n\",\n",
        "      x_expert2.shape,\"\\n\", x_expert3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU-hgjQl1oup",
        "outputId": "e09e38bf-5e85-4c54-e804-4acad3201521"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1999, 4]) \n",
            " torch.Size([500, 4]) \n",
            " torch.Size([1659, 4]) \n",
            " torch.Size([1659, 4]) \n",
            " torch.Size([1659, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "NX0FIBv72sph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hidden dimension\n",
        "output_dim = 3\n",
        "hidden_dim = 32\n",
        "\n",
        "epochs = 500\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "# Instantiate the experts\n",
        "expert1 = Expert(input_dim, hidden_dim, output_dim)\n",
        "expert2 = Expert(input_dim, hidden_dim, output_dim)\n",
        "expert3 = Expert(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Set up loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizers for experts\n",
        "optimizer_expert1 = optim.Adam(expert1.parameters(), lr=learning_rate)\n",
        "optimizer_expert2 = optim.Adam(expert2.parameters(), lr=learning_rate)\n",
        "optimizer_expert3 = optim.Adam(expert3.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop for expert 1\n",
        "for epoch in range(epochs):\n",
        "    optimizer_expert1.zero_grad()\n",
        "\n",
        "    outputs_expert1 = expert1(x_expert1)\n",
        "\n",
        "    loss_expert1 = criterion(outputs_expert1, y_expert1)\n",
        "    loss_expert1.backward()\n",
        "\n",
        "    optimizer_expert1.step()\n",
        "\n",
        "# Training loop for expert 2\n",
        "for epoch in range(epochs):\n",
        "    optimizer_expert2.zero_grad()\n",
        "\n",
        "    outputs_expert2 = expert2(x_expert2)\n",
        "\n",
        "    loss_expert2 = criterion(outputs_expert2, y_expert2)\n",
        "    loss_expert2.backward()\n",
        "\n",
        "    optimizer_expert2.step()\n",
        "\n",
        "# Training loop for expert 3\n",
        "for epoch in range(epochs):\n",
        "    optimizer_expert3.zero_grad()\n",
        "\n",
        "    outputs_expert3 = expert3(x_expert3)\n",
        "\n",
        "    loss_expert3 = criterion(outputs_expert3, y_expert3)\n",
        "    loss_expert3.backward()"
      ],
      "metadata": {
        "id": "mIbbNi1d2hQH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the MoE model with the trained experts\n",
        "moe_model = MoE([expert1, expert2, expert3])\n",
        "\n",
        "# Train the MoE model\n",
        "optimizer_moe = optim.Adam(moe_model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer_moe.zero_grad()\n",
        "\n",
        "    outputs_moe = moe_model(x_train_moe)\n",
        "\n",
        "    loss_moe = criterion(outputs_moe, y_train_moe)\n",
        "    loss_moe.backward()\n",
        "\n",
        "    optimizer_moe.step()"
      ],
      "metadata": {
        "id": "U6-8-_Kc3nkQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate\n"
      ],
      "metadata": {
        "id": "frXQVrqr33wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate all models\n",
        "def evaluate(model, x, y):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct = (predicted == y).sum().item()\n",
        "        accuracy = correct / len(y)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "xVmvR9bv3yv5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_expert1 = evaluate(expert1, x_test, y_test)\n",
        "accuracy_expert2 = evaluate(expert2, x_test, y_test)\n",
        "accuracy_expert3 = evaluate(expert3, x_test, y_test)\n",
        "accuracy_moe = evaluate(moe_model, x_test, y_test)\n",
        "\n",
        "print(\"Expert 1 Accuracy:\", accuracy_expert1)\n",
        "print(\"Expert 2 Accuracy:\", accuracy_expert2)\n",
        "print(\"Expert 3 Accuracy:\", accuracy_expert3)\n",
        "print(\"Mixture of Experts Accuracy:\", accuracy_moe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trQi_1mV39PH",
        "outputId": "ca958901-7a5c-4614-8948-3d66530e7dcd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expert 1 Accuracy: 0.474\n",
            "Expert 2 Accuracy: 0.51\n",
            "Expert 3 Accuracy: 0.316\n",
            "Mixture of Experts Accuracy: 0.612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YM0-oXSh5BRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}