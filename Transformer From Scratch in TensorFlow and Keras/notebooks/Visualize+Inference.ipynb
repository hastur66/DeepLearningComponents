{"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom tensorflow import convert_to_tensor, string\nfrom tensorflow.keras.layers import Embedding, Layer\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\n\nclass PositionEmbeddingFixedWeights(Layer):\n    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n        self.word_embedding_layer = Embedding(\n            input_dim=vocab_size, output_dim=output_dim,\n            weights=[word_embedding_matrix],\n            trainable=False\n        )\n        self.position_embedding_layer = Embedding(\n            input_dim=sequence_length, output_dim=output_dim,\n            weights=[position_embedding_matrix],\n            trainable=False\n        )\n             \n    def get_position_encoding(self, seq_len, d, n=10000):\n        P = np.zeros((seq_len, d))\n        for k in range(seq_len):\n            for i in np.arange(int(d/2)):\n                denominator = np.power(n, 2*i/d)\n                P[k, 2*i] = np.sin(k/denominator)\n                P[k, 2*i+1] = np.cos(k/denominator)\n        return P\n \n \n    def call(self, inputs):        \n        position_indices = tf.range(tf.shape(inputs)[-1])\n        embedded_words = self.word_embedding_layer(inputs)\n        embedded_indices = self.position_embedding_layer(position_indices)\n        return embedded_words + embedded_indices","metadata":{"cell_id":"6c9919a0655541acac9c3b44021f6701","source_hash":"c3f60b9a","execution_start":1685563377923,"execution_millis":10571,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-05-31 20:02:57.973252: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-31 20:02:58.173222: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-05-31 20:02:58.173257: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-05-31 20:02:58.206417: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-05-31 20:03:00.472449: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-05-31 20:03:00.472522: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-05-31 20:03:00.472530: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import matmul, reshape, shape, transpose, cast, float32\nfrom tensorflow.keras.layers import Dense, Layer\nfrom keras.backend import softmax\n\nclass DotProductAttention(Layer):\n    def __init__(self, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n\n    def call(self, queries, keys, values, d_k, mask=None):\n        scores = matmul(queries, keys, transpose_b=True) / tf.math.sqrt(cast(d_k, float32))\n        \n        if mask is not None:\n            scores += -1e9 * mask\n\n        weight = softmax(scores)\n\n        return matmul(weight, values)\n\nclass MultiHeadAttention(Layer):\n    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.attention = DotProductAttention()\n        self.head = h\n        self.d_k = d_k\n        self.d_v = d_v\n        self.d_model = d_model\n        self.W_q = Dense(d_k)\n        self.W_k = Dense(d_k)\n        self.W_v = Dense(d_v)\n        self.W_o = Dense(d_model)\n\n    def reshape_tensor(self, x, head, flag):\n        if flag:\n            x = reshape(x, shape=(shape(x)[0], shape(x)[1], head, -1))\n            x = transpose(x, perm=(0, 2, 1, 3))\n        else:\n            x = transpose(x, perm=(0, 2, 1, 3))\n            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n        return x\n\n    def call(self, queries, keys, values, mask=None):\n        q_reshaped = self.reshape_tensor(self.W_q(queries), self.head, True)\n        k_reshaped = self.reshape_tensor(self.W_k(keys), self.head, True)\n        v_reshaped = self.reshape_tensor(self.W_v(values), self.head, True)\n\n        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n        \n        output = self.reshape_tensor(o_reshaped, self.head, False)\n\n        return self.W_o(output)","metadata":{"cell_id":"5d421f87306d4212a9c7e0ea5fa2bfc3","source_hash":"29229744","execution_start":1685563395117,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout","metadata":{"cell_id":"5bf107f4bf0644069c291c856733bb03","source_hash":"5b1b7c06","execution_start":1685563397193,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class AddNormalization(Layer):\n    def __init__(self, **kwargs):\n        super(AddNormalization, self).__init__(**kwargs)\n        self.layer_norm = LayerNormalization()\n\n    def call(self, x, sublayer_x):\n        add = x + sublayer_x\n        return self.layer_norm(add)\n\nclass FeedForward(Layer):\n    def __init__(self, d_ff, d_model, **kwargs):\n        super(FeedForward, self).__init__(**kwargs)\n        self.fully_connected1 = Dense(d_ff)\n        self.fully_connected2 = Dense(d_model)\n        self.activation = ReLU()\n\n    def call(self, x):\n        x_fc1 = self.fully_connected1(x)\n        return self.fully_connected2(self.activation(x_fc1))","metadata":{"cell_id":"3c3cc389566d44d89eb00070141775f4","source_hash":"e7481df4","execution_start":1685563398558,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class EncoderLayer(Layer):\n    def __init__(self, sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n        super(EncoderLayer, self).__init__(**kwargs)\n        self.build(input_shape=[None, sequence_length, d_model])\n        self.d_model = d_model\n        self.sequence_length = sequence_length\n        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout1 = Dropout(rate)\n        self.add_norm1 = AddNormalization()\n        self.feed_forward = FeedForward(d_ff, d_model)\n        self.dropout2 = Dropout(rate)\n        self.add_norm2 = AddNormalization()\n\n    def build_graph(self):\n        input_layer = Input(shape=(self.sequence_length, self.d_model))\n        return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n\n    def call(self, x, padding_mask, training):\n        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n\n        multihead_output = self.dropout1(multihead_output, training=training)\n\n        addnorm_output = self.add_norm1(x, multihead_output)\n\n        feedforward_output = self.feed_forward(addnorm_output)\n        feedforward_output = self.dropout2(feedforward_output, training=training)\n\n        return self.add_norm2(addnorm_output, feedforward_output)","metadata":{"cell_id":"eefe2ef6ac1c4579af88de88bc0981e2","source_hash":"952339d9","execution_start":1685563399913,"execution_millis":8,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Encoder(Layer):\n    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n        self.dropout = Dropout(rate)\n        self.encoder_layer = [EncoderLayer(sequence_length,h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n\n    def call(self, input_sentence, padding_mask, training):\n        pos_encoding_output = self.pos_encoding(input_sentence)\n\n        x = self.dropout(pos_encoding_output, training=training)\n\n        for i, layer in enumerate(self.encoder_layer):\n            x = layer(x, padding_mask, training)\n\n        return x","metadata":{"cell_id":"5d53fbf6e2bc48868ee57f106bb0813b","source_hash":"de092d46","execution_start":1685563401186,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# from muti_head_attention import MultiHeadAttention\n# from encoder import AddNormalization, FeedForward","metadata":{"cell_id":"135ccb2cab7442bdbe2239b76a44b00a","source_hash":"e7520e14","execution_start":1685563402492,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Implementing the Decoder Layer","metadata":{"cell_id":"f717b807d1a044b7a07a04fefb1b3909","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"class DecoderLayer(Layer):\n    def __init__(self, sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n        super(DecoderLayer, self).__init__(**kwargs)\n        self.build(input_shape=[None, sequence_length, d_model])\n        self.d_model = d_model\n        self.sequence_length = sequence_length\n        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout1 = Dropout(rate)\n        self.add_norm1 = AddNormalization()\n        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout2 = Dropout(rate)\n        self.add_norm2 = AddNormalization()\n        self.feed_forward = FeedForward(d_ff, d_model)\n        self.dropout3 = Dropout(rate)\n        self.add_norm3 = AddNormalization()\n \n    def build_graph(self):\n        input_layer = Input(shape=(self.sequence_length, self.d_model))\n        return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n    \n    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n        multihead_output1 = self.dropout1(multihead_output1, training=training)\n        addnorm_output1 = self.add_norm1(x, multihead_output1)\n\n        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n        multihead_output2 = self.dropout2(multihead_output2, training=training)\n        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n \n        feedforward_output = self.feed_forward(addnorm_output2)\n        feedforward_output = self.dropout3(feedforward_output, training=training)\n \n        return self.add_norm3(addnorm_output2, feedforward_output)","metadata":{"cell_id":"1b81905a6a7241ee8cb53f0e057a0277","source_hash":"9ad6888d","execution_start":1685563405328,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Implementing the Decoder","metadata":{"cell_id":"94f4e24bf96948a8976fd6c31825832b","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"class Decoder(Layer):\n    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n        self.dropout = Dropout(rate)\n        self.decoder_layer = [DecoderLayer(sequence_length,h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n \n    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n        pos_encoding_output = self.pos_encoding(output_target)\n        x = self.dropout(pos_encoding_output, training=training)\n \n        for i, layer in enumerate(self.decoder_layer):\n            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n \n        return x","metadata":{"cell_id":"9275fbfbabc04f9197c3d55784ff964e","source_hash":"30fafcd0","execution_start":1685563407722,"execution_millis":9,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Test","metadata":{"cell_id":"635e946fe4f540f287abcdbd45c812be","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"from numpy import random\n \ndec_vocab_size = 20  # Vocabulary size for the decoder\ninput_seq_length = 5  # Maximum length of the input sequence\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nn = 6  # Number of layers in the decoder stack\n \nbatch_size = 64  # Batch size from the training process\ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n \ninput_seq = random.random((batch_size, input_seq_length))\nenc_output = random.random((batch_size, input_seq_length, d_model))\n \ndecoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)","metadata":{"cell_id":"97075557c0b54b61a43ed38b25ee91cf","source_hash":"1da710b","execution_start":1685563410063,"execution_millis":230,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(decoder(input_seq, enc_output, None, True))","metadata":{"cell_id":"cd5b25a0e4634cf0bc06b1400cac7b2c","source_hash":"f9d061fd","execution_start":1685563411654,"execution_millis":1146,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-05-31 20:03:31.630499: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-05-31 20:03:31.630536: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-05-31 20:03:31.630559: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-b5f739dc-f641-4c72-a448-d84edd2bf5bd): /proc/driver/nvidia/version does not exist\n2023-05-31 20:03:31.630892: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\ntf.Tensor(\n[[[-1.1566199  -0.14741252 -2.159831   ...  1.5885278  -0.44661966\n   -1.7602389 ]\n  [-1.0989492  -0.18139748 -2.038406   ...  1.5659089  -0.49034077\n   -1.7321726 ]\n  [-1.0789104  -0.24323636 -1.9841096  ...  1.5830424  -0.54554665\n   -1.7790005 ]\n  [-1.1257553  -0.2676692  -2.0357425  ...  1.6419091  -0.5608454\n   -1.8521913 ]\n  [-1.1796114  -0.23505542 -2.1333828  ...  1.6907604  -0.56753796\n   -1.888624  ]]\n\n [[-1.7411377  -0.11200375 -2.4061685  ...  1.5312306  -0.5122799\n   -1.6346089 ]\n  [-1.6694007  -0.13805431 -2.3007293  ...  1.5065482  -0.54882205\n   -1.5936977 ]\n  [-1.6642132  -0.21115416 -2.2507906  ...  1.516862   -0.60053855\n   -1.62113   ]\n  [-1.7157524  -0.26186    -2.3003073  ...  1.5677027  -0.62854123\n   -1.6859933 ]\n  [-1.7478511  -0.2264967  -2.3923886  ...  1.636826   -0.6443934\n   -1.736745  ]]\n\n [[-1.2823839  -0.17929263 -2.3378124  ...  1.1756089  -0.2728031\n   -1.3920304 ]\n  [-1.2179649  -0.20095073 -2.2073486  ...  1.1401007  -0.31060782\n   -1.3484455 ]\n  [-1.2103376  -0.26694167 -2.1566339  ...  1.1543995  -0.34730667\n   -1.3922154 ]\n  [-1.2713531  -0.30383787 -2.2248716  ...  1.1973205  -0.35962334\n   -1.4658302 ]\n  [-1.3200287  -0.26573506 -2.3381846  ...  1.2437067  -0.3685371\n   -1.5038531 ]]\n\n ...\n\n [[-1.3472987  -0.07228481 -1.9853668  ...  1.4076807  -0.4240747\n   -1.9158452 ]\n  [-1.2926948  -0.09868819 -1.8651536  ...  1.3860959  -0.46233556\n   -1.8859159 ]\n  [-1.3001947  -0.16028167 -1.8185182  ...  1.401514   -0.514336\n   -1.9171906 ]\n  [-1.3580544  -0.19860595 -1.8844904  ...  1.4492112  -0.54388267\n   -1.9639357 ]\n  [-1.3838402  -0.15715078 -1.9883764  ...  1.5120305  -0.5434543\n   -1.9931093 ]]\n\n [[-1.5786357  -0.05349039 -2.169226   ...  1.5722033  -0.2333955\n   -1.7394187 ]\n  [-1.5262516  -0.07282729 -2.0552022  ...  1.5568053  -0.27729362\n   -1.708277  ]\n  [-1.5255061  -0.1330901  -2.0113857  ...  1.5666145  -0.32217723\n   -1.7332373 ]\n  [-1.5714813  -0.15763976 -2.0714507  ...  1.6177925  -0.35228696\n   -1.8062891 ]\n  [-1.5893301  -0.10770489 -2.1806476  ...  1.6763123  -0.35991678\n   -1.8606539 ]]\n\n [[-1.635864   -0.13038057 -1.9912792  ...  1.6865882  -0.31531984\n   -1.8125521 ]\n  [-1.5849451  -0.16006066 -1.8581132  ...  1.6610321  -0.36773974\n   -1.7853932 ]\n  [-1.5738193  -0.23268385 -1.8073009  ...  1.6744995  -0.4133427\n   -1.8341563 ]\n  [-1.6189479  -0.2771158  -1.8763309  ...  1.7401809  -0.42403015\n   -1.903373  ]\n  [-1.6394473  -0.24412872 -1.994772   ...  1.8181599  -0.42756438\n   -1.9513187 ]]], shape=(64, 5, 512), dtype=float32)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# from encoder import Encoder\n# from decoder import Decoder\nfrom tensorflow import math, cast, float32, linalg, ones, maximum, newaxis\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense, Input","metadata":{"cell_id":"834585759d7e412aa408951acbcf8726","source_hash":"e62a3ecb","execution_start":1685563414962,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class TransformerModel(Model):\n    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n        super(TransformerModel, self).__init__(**kwargs)\n\n        #setup the encoder\n        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        #setup decoder\n        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        #final dense layer\n        self.model_last_layer = Dense(dec_vocab_size)\n\n    def padding_mask(self, input):\n        mask = math.equal(input, 0)\n        mask = cast(mask, float32)\n\n        return mask[:, newaxis, newaxis, :]\n\n    def lookahed_mask(self, shape):\n        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n        return mask\n\n    def call(self, encoder_input, decoder_input, training):\n        enc_padding_mask = self.padding_mask(encoder_input)\n\n        dec_in_padding_mask = self.padding_mask(decoder_input)\n        dec_in_lookahead_mask = self.lookahed_mask(decoder_input.shape[1])\n        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n\n        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n\n        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n\n        model_output = self.model_last_layer(decoder_output)\n\n        return model_output        ","metadata":{"cell_id":"9137e5419fef4b609503d135b71972db","source_hash":"5a4c5a52","execution_start":1685563416550,"execution_millis":10,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":13},{"cell_type":"code","source":"h = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nn = 6  # Number of layers in the encoder stack\n \ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers","metadata":{"cell_id":"79b05919318e499cbacd5d7ae8001a2e","source_hash":"5f256eaf","execution_start":1685563418297,"execution_millis":9,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":14},{"cell_type":"code","source":"enc_vocab_size = 20 # Vocabulary size for the encoder\ndec_vocab_size = 20 # Vocabulary size for the decoder\n \nenc_seq_length = 5  # Maximum length of the input sequence\ndec_seq_length = 5  # Maximum length of the target sequence","metadata":{"cell_id":"293188a7092349ed95c41d73ecbba8b5","source_hash":"4149a81f","execution_start":1685563419541,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# from model import TransformerModel\n \ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)","metadata":{"cell_id":"4a1aa7a217aa417cb716ca4c747e2a29","source_hash":"e3825f48","execution_start":1685563421128,"execution_millis":198,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":16},{"cell_type":"code","source":"encoder = EncoderLayer(enc_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\nencoder.build_graph().summary()\n \ndecoder = DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\ndecoder.build_graph().summary()","metadata":{"cell_id":"a32bd553fe1942ff9d303ee7ca25edcd","source_hash":"16444627","execution_start":1685563426592,"execution_millis":1402,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 5, 512)]     0           []                               \n                                                                                                  \n multi_head_attention_30 (Multi  (None, 5, 512)      131776      ['input_1[0][0]',                \n HeadAttention)                                                   'input_1[0][0]',                \n                                                                  'input_1[0][0]']                \n                                                                                                  \n dropout_51 (Dropout)           (None, 5, 512)       0           ['multi_head_attention_30[0][0]']\n                                                                                                  \n add_normalization_48 (AddNorma  (None, 5, 512)      1024        ['input_1[0][0]',                \n lization)                                                        'dropout_51[0][0]']             \n                                                                                                  \n feed_forward_18 (FeedForward)  (None, 5, 512)       2099712     ['add_normalization_48[0][0]']   \n                                                                                                  \n dropout_52 (Dropout)           (None, 5, 512)       0           ['feed_forward_18[0][0]']        \n                                                                                                  \n add_normalization_49 (AddNorma  (None, 5, 512)      1024        ['add_normalization_48[0][0]',   \n lization)                                                        'dropout_52[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 2,233,536\nTrainable params: 2,233,536\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"},{"output_type":"error","ename":"TypeError","evalue":"call() missing 2 required positional arguments: 'padding_mask' and 'training'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m encoder\u001b[38;5;241m.\u001b[39mbuild_graph()\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m      4\u001b[0m decoder \u001b[38;5;241m=\u001b[39m DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msummary()\n","Cell \u001b[0;32mIn [8], line 19\u001b[0m, in \u001b[0;36mDecoderLayer.build_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_graph\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     18\u001b[0m     input_layer \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model))\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Model(inputs\u001b[38;5;241m=\u001b[39m[input_layer], outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n","\u001b[0;31mTypeError\u001b[0m: call() missing 2 required positional arguments: 'padding_mask' and 'training'"]}],"execution_count":17},{"cell_type":"markdown","source":"### Transformer model","metadata":{"cell_id":"26ca3bfa65b140e68471e1e1fa5fdb5c","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"# from encoder import Encoder, EncoderLayer\n# from decoder import Decoder, DecoderLayer\nfrom tensorflow import math, cast, float32, linalg, ones, maximum, newaxis\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense\n \n \nclass TransformerModel(Model):\n    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n        super(TransformerModel, self).__init__(**kwargs)\n\n        self.encoder_layer = EncoderLayer(enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, rate)\n        self.decoder_layer = DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, rate)\n \n        # Set up the encoder\n        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n \n        # Set up the decoder\n        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n \n        # Define the final dense layer\n        self.model_last_layer = Dense(dec_vocab_size)\n \n    def padding_mask(self, input):\n        # Create mask which marks the zero padding values in the input by a 1.0\n        mask = math.equal(input, 0)\n        mask = cast(mask, float32)\n \n        # The shape of the mask should be broadcastable to the shape\n        # of the attention weights that it will be masking later on\n        return mask[:, newaxis, newaxis, :]\n \n    def lookahead_mask(self, shape):\n        # Mask out future entries by marking them with a 1.0\n        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n \n        return mask\n    \n    def model_summary(self):\n        # Show model summary\n        encoder_summary = self.encoder_layer.build_graph().summary()\n        decoder_summary = self.decoder_layer.build_graph().summary()\n        return encoder_summary, decoder_summary\n \n    def call(self, encoder_input, decoder_input, training):\n \n        # Create padding mask to mask the encoder inputs and the encoder outputs in the decoder\n        enc_padding_mask = self.padding_mask(encoder_input)\n \n        # Create and combine padding and look-ahead masks to be fed into the decoder\n        dec_in_padding_mask = self.padding_mask(decoder_input)\n        dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n \n        # Feed the input into the encoder\n        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n \n        # Feed the encoder output into the decoder\n        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n \n        # Pass the decoder output through a final dense layer\n        model_output = self.model_last_layer(decoder_output)\n \n        return model_output\n    ","metadata":{"cell_id":"32d835cd023f48e1b6cbd2a6716038aa","source_hash":"993d4494","execution_start":1685563854105,"execution_millis":7,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Prepare dataset","metadata":{"cell_id":"160a7521b71e4b93ba7c6d265fe4ee1c","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"from pickle import load, dump, HIGHEST_PROTOCOL\nfrom numpy.random import shuffle\nfrom numpy import savetxt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\nfrom tensorflow  import convert_to_tensor, int64","metadata":{"cell_id":"0986102546fc4aa18624e5ea21fd8998","source_hash":"f4f8e54","execution_start":1685563855529,"execution_millis":9,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class PrepareDataset:\n    def __init__(self, **kwargs):\n        super(PrepareDataset, self).__init__(**kwargs)\n        self.n_sentences = 15000  # Number of sentences to include in the dataset\n        self.train_split = 0.8  # Ratio of the training data split\n        self.val_split = 0.1  # Ratio of the validation data split\n \n    # Fit a tokenizer\n    def create_tokenizer(self, dataset):\n        tokenizer = Tokenizer()\n        tokenizer.fit_on_texts(dataset)\n \n        return tokenizer\n \n    def find_seq_length(self, dataset):\n        return max(len(seq.split()) for seq in dataset)\n \n    def find_vocab_size(self, tokenizer, dataset):\n        tokenizer.fit_on_texts(dataset)\n \n        return len(tokenizer.word_index) + 1\n \n    # Encode and pad the input sequences\n    def encode_pad(self, dataset, tokenizer, seq_length):\n        x = tokenizer.texts_to_sequences(dataset)\n        x = pad_sequences(x, maxlen=seq_length, padding='post')\n        x = convert_to_tensor(x, dtype=int64)\n \n        return x\n \n    def save_tokenizer(self, tokenizer, name):\n        with open(name + '_tokenizer.pkl', 'wb') as handle:\n            dump(tokenizer, handle, protocol=HIGHEST_PROTOCOL)\n \n    def __call__(self, filename, **kwargs):\n        # Load a clean dataset\n        clean_dataset = load(open(filename, 'rb'))\n \n        # Reduce dataset size\n        dataset = clean_dataset[:self.n_sentences, :]\n \n        # Include start and end of string tokens\n        for i in range(dataset[:, 0].size):\n            dataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n            dataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n \n        # Random shuffle the dataset\n        shuffle(dataset)\n \n        # Split the dataset in training, validation and test sets\n        train = dataset[:int(self.n_sentences * self.train_split)]\n        val = dataset[int(self.n_sentences * self.train_split):int(self.n_sentences * (1-self.val_split))]\n        test = dataset[int(self.n_sentences * (1 - self.val_split)):]\n \n        # Prepare tokenizer for the encoder input\n        enc_tokenizer = self.create_tokenizer(dataset[:, 0])\n        enc_seq_length = self.find_seq_length(dataset[:, 0])\n        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n \n        # Prepare tokenizer for the decoder input\n        dec_tokenizer = self.create_tokenizer(dataset[:, 1])\n        dec_seq_length = self.find_seq_length(dataset[:, 1])\n        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n \n        # Encode and pad the training input\n        trainX = self.encode_pad(train[:, 0], enc_tokenizer, enc_seq_length)\n        trainY = self.encode_pad(train[:, 1], dec_tokenizer, dec_seq_length)\n \n        # Encode and pad the validation input\n        valX = self.encode_pad(val[:, 0], enc_tokenizer, enc_seq_length)\n        valY = self.encode_pad(val[:, 1], dec_tokenizer, dec_seq_length)\n \n        # Save the encoder tokenizer\n        self.save_tokenizer(enc_tokenizer, 'enc')\n \n        # Save the decoder tokenizer\n        self.save_tokenizer(dec_tokenizer, 'dec')\n \n        # Save the testing dataset into a text file\n        savetxt('test_dataset.txt', test, fmt='%s')\n \n        return trainX, trainY, valX, valY, train, val, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size\n","metadata":{"cell_id":"3f2b753dd5924d63a28070c4dc377e49","source_hash":"63a9a35e","execution_start":1685563857195,"execution_millis":8,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Prepare the training data\ndataset = PrepareDataset()\ntrainX, trainY, valX, valY, train, val, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n \nprint(train[0, 0], '\\n', trainX[0, :])","metadata":{"cell_id":"28000f13a5d44c149cbb012231d18986","source_hash":"e168ec5a","execution_start":1685563998735,"execution_millis":989,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"<START> how romantic <EOS> \n tf.Tensor([   1   48 1521    2    0    0    0], shape=(7,), dtype=int64)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print('Encoder sequence length:', enc_seq_length)","metadata":{"cell_id":"bcc3fde0170f46b2bd6490b9fa0a9a21","source_hash":"d40cf17d","execution_start":1685564004216,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Encoder sequence length: 7\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(train[0, 1], '\\n', trainY[0, :])","metadata":{"cell_id":"fb3fd9bf59544a0ebf470128fac2d5c6","source_hash":"5ddf21cd","execution_start":1685564011180,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"<START> wie romantisch <EOS> \n tf.Tensor([   1   36 2012    2    0    0    0    0    0    0    0    0], shape=(12,), dtype=int64)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"print('Decoder sequence length:', dec_seq_length)","metadata":{"cell_id":"d183de61f4e648709ef31967aef96b5d","source_hash":"dd2bba8c","execution_start":1685564014041,"execution_millis":12,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Decoder sequence length: 12\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"### Train model","metadata":{"cell_id":"16854aabb9c84d6a9987c6a643dbc76e","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import LearningRateSchedule\nfrom tensorflow.keras.metrics import Mean\nfrom tensorflow import data, train, math, reduce_sum, cast, equal, argmax, float32, GradientTape, function\nfrom keras.losses import sparse_categorical_crossentropy\nfrom time import time\nfrom pickle import dump\n \n \n# Define the model parameters\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of model layers' outputs\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nn = 6  # Number of layers in the encoder stack\n \n# Define the training parameters\nepochs = 20\nbatch_size = 64\nbeta_1 = 0.9\nbeta_2 = 0.98\nepsilon = 1e-9\ndropout_rate = 0.1\n \n \n# Implementing a learning rate scheduler\nclass LRScheduler(LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n        super(LRScheduler, self).__init__(**kwargs)\n \n        self.d_model = cast(d_model, float32)\n        self.warmup_steps = warmup_steps\n \n    def __call__(self, step_num):\n \n        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n        arg1 = step_num ** -0.5\n        arg2 = step_num * (self.warmup_steps ** -1.5)\n \n        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n \n \n# Instantiate an Adam optimizer\noptimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n \n# Prepare the training dataset\ndataset = PrepareDataset()\ntrainX, trainY, valX, valY, train_orig, val_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n \nprint(enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size)\n \n# Prepare the training dataset batches\ntrain_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\ntrain_dataset = train_dataset.batch(batch_size)\n \n# Prepare the validation dataset batches\nval_dataset = data.Dataset.from_tensor_slices((valX, valY))\nval_dataset = val_dataset.batch(batch_size)\n \n# Create model\ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n \n \n# Defining the loss function\ndef loss_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of loss\n    padding_mask = math.logical_not(equal(target, 0))\n    padding_mask = cast(padding_mask, float32)\n \n    # Compute a sparse categorical cross-entropy loss on the unmasked values\n    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n \n    # Compute the mean loss over the unmasked values\n    return reduce_sum(loss) / reduce_sum(padding_mask)\n \n \n# Defining the accuracy function\ndef accuracy_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of accuracy\n    padding_mask = math.logical_not(equal(target, 0))\n \n    # Find equal prediction and target values, and apply the padding mask\n    accuracy = equal(target, argmax(prediction, axis=2))\n    accuracy = math.logical_and(padding_mask, accuracy)\n \n    # Cast the True/False values to 32-bit-precision floating-point numbers\n    padding_mask = cast(padding_mask, float32)\n    accuracy = cast(accuracy, float32)\n \n    # Compute the mean accuracy over the unmasked values\n    return reduce_sum(accuracy) / reduce_sum(padding_mask)\n \n \n# Include metrics monitoring\ntrain_loss = Mean(name='train_loss')\ntrain_accuracy = Mean(name='train_accuracy')\nval_loss = Mean(name='val_loss')\n \n# Create a checkpoint object and manager to manage multiple checkpoints\nckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\nckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=None)\n \n# Initialise dictionaries to store the training and validation losses\ntrain_loss_dict = {}\nval_loss_dict = {}\n \n# Speeding up the training process\n@function\ndef train_step(encoder_input, decoder_input, decoder_output):\n    with GradientTape() as tape:\n \n        # Run the forward pass of the model to generate a prediction\n        prediction = training_model(encoder_input, decoder_input, training=True)\n \n        # Compute the training loss\n        loss = loss_fcn(decoder_output, prediction)\n \n        # Compute the training accuracy\n        accuracy = accuracy_fcn(decoder_output, prediction)\n \n    # Retrieve gradients of the trainable variables with respect to the training loss\n    gradients = tape.gradient(loss, training_model.trainable_weights)\n \n    # Update the values of the trainable variables by gradient descent\n    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n \n    train_loss(loss)\n    train_accuracy(accuracy)\n \n \nfor epoch in range(epochs):\n \n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    val_loss.reset_states()\n \n    print(\"\\nStart of epoch %d\" % (epoch + 1))\n \n    start_time = time()\n \n    # Iterate over the dataset batches\n    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n \n        # Define the encoder and decoder inputs, and the decoder output\n        encoder_input = train_batchX[:, 1:]\n        decoder_input = train_batchY[:, :-1]\n        decoder_output = train_batchY[:, 1:]\n \n        train_step(encoder_input, decoder_input, decoder_output)\n \n        if step % 50 == 0:\n            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n \n    # Run a validation step after every epoch of training\n    for val_batchX, val_batchY in val_dataset:\n \n        # Define the encoder and decoder inputs, and the decoder output\n        encoder_input = val_batchX[:, 1:]\n        decoder_input = val_batchY[:, :-1]\n        decoder_output = val_batchY[:, 1:]\n \n        # Generate a prediction\n        prediction = training_model(encoder_input, decoder_input, training=False)\n \n        # Compute the validation loss\n        loss = loss_fcn(decoder_output, prediction)\n        val_loss(loss)\n \n    # Print epoch number and accuracy and loss values at the end of every epoch\n    print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f, Validation Loss %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result(), val_loss.result()))\n \n    # Save a checkpoint after every epoch\n    if (epoch + 1) % 1 == 0:\n \n        save_path = ckpt_manager.save()\n        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))\n \n        # Save the trained model weights\n        training_model.save_weights(\"weights/wghts\" + str(epoch + 1) + \".ckpt\")\n \n        train_loss_dict[epoch] = train_loss.result()\n        val_loss_dict[epoch] = val_loss.result()\n \n# Save the training loss values\nwith open('./train_loss.pkl', 'wb') as file:\n    dump(train_loss_dict, file)\n \n# Save the validation loss values\nwith open('./val_loss.pkl', 'wb') as file:\n    dump(val_loss_dict, file)\n \nprint(\"Total time taken: %.2fs\" % (time() - start_time))","metadata":{"cell_id":"7f6d03418f99437a96e961c36c18a756","source_hash":"909b0f1a","execution_start":1685564227743,"execution_millis":825034,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"7 12 2404 3864\n\nStart of epoch 1\nEpoch 1 Step 0 Loss 8.3835 Accuracy 0.0000\nEpoch 1 Step 50 Loss 7.5832 Accuracy 0.1395\nEpoch 1 Step 100 Loss 6.9980 Accuracy 0.1797\nEpoch 1 Step 150 Loss 6.6255 Accuracy 0.2019\n2023-05-31 20:23:53.159081: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\nEpoch 1: Training Loss 6.5834, Training Accuracy 0.2049, Validation Loss 0.0000\nSaved checkpoint at epoch 1\n\nStart of epoch 2\nEpoch 2 Step 0 Loss 5.4997 Accuracy 0.2724\nEpoch 2 Step 50 Loss 5.3582 Accuracy 0.2743\nEpoch 2 Step 100 Loss 5.2092 Accuracy 0.2831\nEpoch 2 Step 150 Loss 5.0632 Accuracy 0.2924\nEpoch 2: Training Loss 5.0474, Training Accuracy 0.2940, Validation Loss 0.0000\nSaved checkpoint at epoch 2\n\nStart of epoch 3\nEpoch 3 Step 0 Loss 4.6026 Accuracy 0.3172\n","output_type":"stream"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [35], line 150\u001b[0m\n\u001b[1;32m    147\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m train_batchY[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    148\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m train_batchY[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 150\u001b[0m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"execution_count":35},{"cell_type":"markdown","source":"### Visualize","metadata":{"cell_id":"b8f1e42e1d784943985184aedefd80b1","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"from pickle import load\nfrom matplotlib.pylab import plt\nfrom numpy import arange\n \n# Load the training and validation loss dictionaries\ntrain_loss = load(open('train_loss.pkl', 'rb'))\nval_loss = load(open('val_loss.pkl', 'rb'))\n \n# Retrieve each dictionary's values\ntrain_values = train_loss.values()\nval_values = val_loss.values()\n \n# Generate a sequence of integers to represent the epoch numbers\nepochs = range(1, 21)\n \n# Plot and label the training and validation loss values\nplt.plot(epochs, train_values, label='Training Loss')\nplt.plot(epochs, val_values, label='Validation Loss')\n \n# Add in a title and axes labels\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n \n# Set the tick locations\nplt.xticks(arange(0, 21, 2))\n \n# Display the plot\nplt.legend(loc='best')\nplt.show()","metadata":{"cell_id":"83b89f5a85f747299a16b864de755a30","source_hash":"2daeda84","execution_start":1685565060424,"execution_millis":17,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'train_loss.pkl'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [36], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arange\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the training and validation loss dictionaries\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_loss.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Retrieve each dictionary's values\u001b[39;00m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_loss.pkl'"]}],"execution_count":36},{"cell_type":"markdown","source":"### Inference","metadata":{"cell_id":"8b996944e033481d834bec2b02025140","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"from pickle import load\nfrom tensorflow import Module\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import convert_to_tensor, int64, TensorArray, argmax, newaxis, transpose\nfrom model import TransformerModel\n \n# Define the model parameters\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of model layers' outputs\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nn = 6  # Number of layers in the encoder stack\n \n# Define the dataset parameters\nenc_seq_length = 7  # Encoder sequence length\ndec_seq_length = 12  # Decoder sequence length\nenc_vocab_size = 2405  # Encoder vocabulary size\ndec_vocab_size = 3858  # Decoder vocabulary size\n \n# Create model\ninferencing_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, 0)\n \n \nclass Translate(Module):\n    def __init__(self, inferencing_model, **kwargs):\n        super(Translate, self).__init__(**kwargs)\n        self.transformer = inferencing_model\n \n    def load_tokenizer(self, name):\n        with open(name, 'rb') as handle:\n            return load(handle)\n \n    def __call__(self, sentence):\n        # Append start and end of string tokens to the input sentence\n        sentence[0] = \"<START> \" + sentence[0] + \" <EOS>\"\n \n        # Load encoder and decoder tokenizers\n        enc_tokenizer = self.load_tokenizer('enc_tokenizer.pkl')\n        dec_tokenizer = self.load_tokenizer('dec_tokenizer.pkl')\n \n        # Prepare the input sentence by tokenizing, padding and converting to tensor\n        encoder_input = enc_tokenizer.texts_to_sequences(sentence)\n        encoder_input = pad_sequences(encoder_input, maxlen=enc_seq_length, padding='post')\n        encoder_input = convert_to_tensor(encoder_input, dtype=int64)\n \n        # Prepare the output <START> token by tokenizing, and converting to tensor\n        output_start = dec_tokenizer.texts_to_sequences([\"<START>\"])\n        output_start = convert_to_tensor(output_start[0], dtype=int64)\n \n        # Prepare the output <EOS> token by tokenizing, and converting to tensor\n        output_end = dec_tokenizer.texts_to_sequences([\"<EOS>\"])\n        output_end = convert_to_tensor(output_end[0], dtype=int64)\n \n        # Prepare the output array of dynamic size\n        decoder_output = TensorArray(dtype=int64, size=0, dynamic_size=True)\n        decoder_output = decoder_output.write(0, output_start)\n \n        for i in range(dec_seq_length):\n \n            # Predict an output token\n            prediction = self.transformer(encoder_input, transpose(decoder_output.stack()), training=False)\n \n            prediction = prediction[:, -1, :]\n \n            # Select the prediction with the highest score\n            predicted_id = argmax(prediction, axis=-1)\n            predicted_id = predicted_id[0][newaxis]\n \n            # Write the selected prediction to the output array at the next available index\n            decoder_output = decoder_output.write(i + 1, predicted_id)\n \n            # Break if an <EOS> token is predicted\n            if predicted_id == output_end:\n                break\n \n        output = transpose(decoder_output.stack())[0]\n        output = output.numpy()\n \n        output_str = []\n \n        # Decode the predicted tokens into an output string\n        for i in range(output.shape[0]):\n \n            key = output[i]\n            print(dec_tokenizer.index_word[key])\n \n        return output_str","metadata":{"cell_id":"ad8ab938404647c495652e11635f9af5","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b5f739dc-f641-4c72-a448-d84edd2bf5bd' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"85d86b6b92ed41f7ad226c91a89060ca","deepnote_execution_queue":[]}}