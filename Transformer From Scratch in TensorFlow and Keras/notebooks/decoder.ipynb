{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ae3da5f0c5804a48b516d6d2284ec44b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6488,"execution_start":1684434628072,"source_hash":"c3f60b9a"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","from tensorflow import convert_to_tensor, string\n","from tensorflow.keras.layers import Embedding, Layer, LayerNormalization, Dense, ReLU, Dropout\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","from tensorflow import matmul, reshape, shape, transpose, cast, float32\n","from keras.backend import softmax"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PositionEmbeddingFixedWeights(Layer):\n","    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n","        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n","        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n","        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n","        self.word_embedding_layer = Embedding(\n","            input_dim=vocab_size, output_dim=output_dim,\n","            weights=[word_embedding_matrix],\n","            trainable=False\n","        )\n","        self.position_embedding_layer = Embedding(\n","            input_dim=sequence_length, output_dim=output_dim,\n","            weights=[position_embedding_matrix],\n","            trainable=False\n","        )\n","             \n","    def get_position_encoding(self, seq_len, d, n=10000):\n","        P = np.zeros((seq_len, d))\n","        for k in range(seq_len):\n","            for i in np.arange(int(d/2)):\n","                denominator = np.power(n, 2*i/d)\n","                P[k, 2*i] = np.sin(k/denominator)\n","                P[k, 2*i+1] = np.cos(k/denominator)\n","        return P\n"," \n"," \n","    def call(self, inputs):        \n","        position_indices = tf.range(tf.shape(inputs)[-1])\n","        embedded_words = self.word_embedding_layer(inputs)\n","        embedded_indices = self.position_embedding_layer(position_indices)\n","        return embedded_words + embedded_indices"]},{"cell_type":"code","execution_count":25,"metadata":{"cell_id":"5ef76ef69d1f44f2aa1e8b007a879449","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1684437488737,"source_hash":"29229744"},"outputs":[],"source":["class DotProductAttention(Layer):\n","    def __init__(self, **kwargs):\n","        super(DotProductAttention, self).__init__(**kwargs)\n","\n","    def call(self, queries, keys, values, d_k, mask=None):\n","        scores = matmul(queries, keys, transpose_b=True) / tf.math.sqrt(cast(d_k, float32))\n","        \n","        if mask is not None:\n","            scores += -1e9 * mask\n","\n","        weight = softmax(scores)\n","\n","        return matmul(weight, values)\n","\n","class MultiHeadAttention(Layer):\n","    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n","        super(MultiHeadAttention, self).__init__(**kwargs)\n","        self.attention = DotProductAttention()\n","        self.head = h\n","        self.d_k = d_k\n","        self.d_v = d_v\n","        self.d_model = d_model\n","        self.W_q = Dense(d_k)\n","        self.W_k = Dense(d_k)\n","        self.W_v = Dense(d_v)\n","        self.W_o = Dense(d_model)\n","\n","    def reshape_tensor(self, x, head, flag):\n","        if flag:\n","            x = reshape(x, shape=(shape(x)[0], shape(x)[1], head, -1))\n","            x = transpose(x, perm=(0, 2, 1, 3))\n","        else:\n","            x = transpose(x, perm=(0, 2, 1, 3))\n","            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n","        return x\n","\n","    def call(self, queries, keys, values, mask=None):\n","        q_reshaped = self.reshape_tensor(self.W_q(queries), self.head, True)\n","        k_reshaped = self.reshape_tensor(self.W_k(keys), self.head, True)\n","        v_reshaped = self.reshape_tensor(self.W_v(values), self.head, True)\n","\n","        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n","        \n","        output = self.reshape_tensor(o_reshaped, self.head, False)\n","\n","        return self.W_o(output)"]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"872098e6c0dc4a5a96da2812aa04f61e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1684434665087,"source_hash":"e7481df4"},"outputs":[],"source":["class AddNormalization(Layer):\n","    def __init__(self, **kwargs):\n","        super(AddNormalization, self).__init__(**kwargs)\n","        self.layer_norm = LayerNormalization()\n","\n","    def call(self, x, sublayer_x):\n","        add = x + sublayer_x\n","        return self.layer_norm(add)\n","\n","class FeedForward(Layer):\n","    def __init__(self, d_ff, d_model, **kwargs):\n","        super(FeedForward, self).__init__(**kwargs)\n","        self.fully_connected1 = Dense(d_ff)\n","        self.fully_connected2 = Dense(d_model)\n","        self.activation = ReLU()\n","\n","    def call(self, x):\n","        x_fc1 = self.fully_connected1(x)\n","        return self.fully_connected2(self.activation(x_fc1))"]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"3c365ad3f43843c99027f574a936a2cf","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1684434666839,"source_hash":"48d4481b"},"outputs":[],"source":["class EncoderLayer(Layer):\n","    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n","        super(EncoderLayer, self).__init__(**kwargs)\n","        self.muti_head_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n","        self.dropout1 = Dropout(rate)\n","        self.add_norm1 = AddNormalization()\n","        self.feed_forward = FeedForward(d_ff, d_model)\n","        self.dropout2 = Dropout(rate)\n","        self.add_norm2 = AddNormalization()\n","\n","    def call(self, x, padding_mask, training):\n","        multihead_output = self.muti_head_attention(x, x, x, padding_mask)\n","        multihead_output = self.dropout1(multihead_output, training=training)\n","        addnorm_output = self.add_norm1(x, multihead_output)\n","        feedforward_output = self.feed_forward(addnorm_output)\n","        feedforward_output = self.dropout2(feedforward_output, training=training)\n","        return self.add_norm2(addnorm_output, feedforward_output)"]},{"cell_type":"code","execution_count":26,"metadata":{"cell_id":"4071b6d6c52048f789749b8527aa1919","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1684437493857,"source_hash":"a11265f4"},"outputs":[],"source":["class Encoder(Layer):\n","    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n","        super(Encoder, self).__init__(**kwargs)\n","        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n","        self.dropout = Dropout(rate)\n","        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n","\n","    def call(self, input_sentence, padding_mask, training):\n","        pos_encoding_output = self.pos_encoding(input_sentence)\n","        x = self.dropout(pos_encoding_output, training=training)\n","\n","        for i, layer in enumerate(self.encoder_layer):\n","            x = layer(x, padding_mask, training)\n","\n","        return x"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"5e65c2d1c6fb41ddaafbf5add6d8d607","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Implementing the Decoder Layer"]},{"cell_type":"code","execution_count":27,"metadata":{"cell_id":"738af74ddde04a269435af7b3cc8848e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1684437497258,"source_hash":"27a191a3"},"outputs":[],"source":["class DecoderLayer(Layer):\n","    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n","        super(DecoderLayer, self).__init__(**kwargs)\n","        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n","        self.dropout1 = Dropout(rate)\n","        self.add_norm1 = AddNormalization()\n","        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n","        self.dropout2 = Dropout(rate)\n","        self.add_norm2 = AddNormalization()\n","        self.feed_forward = FeedForward(d_ff, d_model)\n","        self.dropout3 = Dropout(rate)\n","        self.add_norm3 = AddNormalization()\n","\n","    def call(self, x, encoder_output, lookahed_mask, padding_mask, training):\n","        multihead_output1 = self.multihead_attention1(x, x, x, lookahed_mask)\n","        multihead_output1 = self.dropout1(multihead_output1, training=training)\n","        addnorm_output1 = self.add_norm1(x, multihead_output1)\n","\n","        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n","        multihead_output2 = self.dropout2(multihead_output2, training=training)\n","        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n","        \n","        feedforward_output = self.feed_forward(addnorm_output2)\n","        feedforward_output = self.dropout3(feedforward_output, training=training)\n"," \n","        return self.add_norm3(addnorm_output2, feedforward_output)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"ce5800d82c834818b79e2cb711162744","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Implementing the Decoder"]},{"cell_type":"code","execution_count":28,"metadata":{"cell_id":"841219ee69d1450a9179d091bc0ac1dd","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1684437499902,"source_hash":"b2ef6184"},"outputs":[],"source":["class Decoder(Layer):\n","    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n","        super(Decoder, self).__init__(**kwargs)\n","        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n","        self.dropout = Dropout(rate)\n","        self.decoder_layer = [DecoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n","\n","    def call(self, output_target, encoder_output, lookahed_mask, padding_mask, training):\n","        pos_encoding_output = self.pos_encoding(output_target)\n","        x = self.dropout(pos_encoding_output, training=training)\n","\n","        for i, layer in enumerate(self.decoder_layer):\n","            x = layer(x, encoder_output, lookahed_mask, padding_mask, training)\n","\n","        return x"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"f885a09dd3fd486eb0d7d6bf3f66a4e4","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Test"]},{"cell_type":"code","execution_count":29,"metadata":{"cell_id":"29d4b6569ecc4ec0b90817594a071a06","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":128,"execution_start":1684437502668,"source_hash":"1da710b"},"outputs":[],"source":["from numpy import random\n"," \n","dec_vocab_size = 20  # Vocabulary size for the decoder\n","input_seq_length = 5  # Maximum length of the input sequence\n","h = 8  # Number of self-attention heads\n","d_k = 64  # Dimensionality of the linearly projected queries and keys\n","d_v = 64  # Dimensionality of the linearly projected values\n","d_ff = 2048  # Dimensionality of the inner fully connected layer\n","d_model = 512  # Dimensionality of the model sub-layers' outputs\n","n = 6  # Number of layers in the decoder stack\n"," \n","batch_size = 64  # Batch size from the training process\n","dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n"," \n","input_seq = random.random((batch_size, input_seq_length))\n","enc_output = random.random((batch_size, input_seq_length, d_model))\n"," \n","decoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)"]},{"cell_type":"code","execution_count":30,"metadata":{"cell_id":"0663de5ed66e4fc39790be15acfb5726","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":665,"execution_start":1684437507194,"source_hash":"f9d061fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[[[-0.04305046 -0.47484383  0.7590988  ... -1.2985758  -0.5103778\n","   -0.18634765]\n","  [-0.0173292  -0.5478449   0.8449123  ... -1.3009796  -0.45211256\n","   -0.12623613]\n","  [-0.05238974 -0.6602539   0.89172536 ... -1.2963398  -0.39726332\n","   -0.10536609]\n","  [-0.14152175 -0.7307735   0.8529401  ... -1.2855538  -0.34921044\n","   -0.12891695]\n","  [-0.22316211 -0.71184945  0.772886   ... -1.2889701  -0.3244501\n","   -0.15551025]]\n","\n"," [[ 0.07582311 -0.5314726   1.0726547  ... -1.4293619  -0.6895368\n","   -0.08790114]\n","  [ 0.10653358 -0.60402477  1.1682818  ... -1.4241368  -0.6409121\n","   -0.02487357]\n","  [ 0.07467948 -0.73259     1.2064568  ... -1.4333439  -0.61553365\n","   -0.01072321]\n","  [-0.01001988 -0.8275769   1.1622007  ... -1.4422469  -0.60315615\n","   -0.03931875]\n","  [-0.09561822 -0.7998715   1.1051866  ... -1.4587178  -0.5895642\n","   -0.08978521]]\n","\n"," [[-0.01351693 -0.31200728  0.99444175 ... -1.240911   -0.64548874\n","   -0.254026  ]\n","  [ 0.0023868  -0.3882196   1.0818783  ... -1.2426058  -0.59460956\n","   -0.21101643]\n","  [-0.02889756 -0.4996297   1.1287749  ... -1.2386373  -0.56580555\n","   -0.20635626]\n","  [-0.10098945 -0.5741602   1.0935298  ... -1.2348874  -0.5408753\n","   -0.24041526]\n","  [-0.1706658  -0.5665265   1.0214837  ... -1.2461926  -0.5280501\n","   -0.28350493]]\n","\n"," ...\n","\n"," [[ 0.0955133  -0.24166389  1.318186   ... -1.2175804  -0.7083104\n","   -0.05880136]\n","  [ 0.11817686 -0.3069289   1.4063252  ... -1.2200556  -0.63875604\n","   -0.00600447]\n","  [ 0.08070242 -0.41751862  1.4250749  ... -1.2376384  -0.59300834\n","   -0.01158342]\n","  [-0.01090575 -0.49744672  1.3615555  ... -1.259051   -0.54979414\n","   -0.04426106]\n","  [-0.11169886 -0.4737375   1.2740586  ... -1.2846298  -0.5175345\n","   -0.09027123]]\n","\n"," [[ 0.01860209 -0.07326771  0.8672447  ... -1.7359005  -0.6641492\n","   -0.19596134]\n","  [ 0.05162951 -0.14441766  0.9488604  ... -1.728869   -0.618183\n","   -0.14336345]\n","  [ 0.02144552 -0.27782667  0.9911281  ... -1.7179317  -0.5810506\n","   -0.13882314]\n","  [-0.08063881 -0.36194205  0.9374526  ... -1.7272964  -0.55611944\n","   -0.18866138]\n","  [-0.17513081 -0.3476701   0.86735505 ... -1.7592272  -0.54203224\n","   -0.23128438]]\n","\n"," [[ 0.21908885 -0.15603405  1.2499603  ... -1.4380971  -0.6972676\n","    0.07085018]\n","  [ 0.24465343 -0.22274926  1.3361661  ... -1.4385312  -0.6480312\n","    0.12395713]\n","  [ 0.20382228 -0.34320492  1.3794463  ... -1.4381539  -0.6036274\n","    0.14168157]\n","  [ 0.11239218 -0.42170566  1.3384007  ... -1.4406346  -0.5640678\n","    0.11538233]\n","  [ 0.0412784  -0.38871944  1.2760551  ... -1.4621195  -0.5464059\n","    0.07616047]]], shape=(64, 5, 512), dtype=float32)\n"]}],"source":["print(decoder(input_seq, enc_output, None, True))"]},{"attachments":{},"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b5f739dc-f641-4c72-a448-d84edd2bf5bd' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"55df7e676d6349ab9b1e319fd705a2c6","language_info":{"name":"python"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
