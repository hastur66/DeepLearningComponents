{"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom tensorflow import convert_to_tensor, string\nfrom tensorflow.keras.layers import Embedding, Layer\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\n\nclass PositionEmbeddingFixedWeights(Layer):\n    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n        self.word_embedding_layer = Embedding(\n            input_dim=vocab_size, output_dim=output_dim,\n            weights=[word_embedding_matrix],\n            trainable=False\n        )\n        self.position_embedding_layer = Embedding(\n            input_dim=sequence_length, output_dim=output_dim,\n            weights=[position_embedding_matrix],\n            trainable=False\n        )\n             \n    def get_position_encoding(self, seq_len, d, n=10000):\n        P = np.zeros((seq_len, d))\n        for k in range(seq_len):\n            for i in np.arange(int(d/2)):\n                denominator = np.power(n, 2*i/d)\n                P[k, 2*i] = np.sin(k/denominator)\n                P[k, 2*i+1] = np.cos(k/denominator)\n        return P\n \n \n    def call(self, inputs):        \n        position_indices = tf.range(tf.shape(inputs)[-1])\n        embedded_words = self.word_embedding_layer(inputs)\n        embedded_indices = self.position_embedding_layer(position_indices)\n        return embedded_words + embedded_indices","metadata":{"cell_id":"69d7d3b36a1d43e9ba7844c3976786e7","source_hash":"c3f60b9a","execution_start":1685381956547,"execution_millis":7437,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-05-29 17:39:16.537921: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-29 17:39:16.874684: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-05-29 17:39:16.874719: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-05-29 17:39:16.910727: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-05-29 17:39:18.783029: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-05-29 17:39:18.783113: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-05-29 17:39:18.783122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import matmul, reshape, shape, transpose, cast, float32\nfrom tensorflow.keras.layers import Dense, Layer\nfrom keras.backend import softmax\n\nclass DotProductAttention(Layer):\n    def __init__(self, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n\n    def call(self, queries, keys, values, d_k, mask=None):\n        scores = matmul(queries, keys, transpose_b=True) / tf.math.sqrt(cast(d_k, float32))\n        \n        if mask is not None:\n            scores += -1e9 * mask\n\n        weight = softmax(scores)\n\n        return matmul(weight, values)\n\nclass MultiHeadAttention(Layer):\n    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.attention = DotProductAttention()\n        self.head = h\n        self.d_k = d_k\n        self.d_v = d_v\n        self.d_model = d_model\n        self.W_q = Dense(d_k)\n        self.W_k = Dense(d_k)\n        self.W_v = Dense(d_v)\n        self.W_o = Dense(d_model)\n\n    def reshape_tensor(self, x, head, flag):\n        if flag:\n            x = reshape(x, shape=(shape(x)[0], shape(x)[1], head, -1))\n            x = transpose(x, perm=(0, 2, 1, 3))\n        else:\n            x = transpose(x, perm=(0, 2, 1, 3))\n            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n        return x\n\n    def call(self, queries, keys, values, mask=None):\n        q_reshaped = self.reshape_tensor(self.W_q(queries), self.head, True)\n        k_reshaped = self.reshape_tensor(self.W_k(keys), self.head, True)\n        v_reshaped = self.reshape_tensor(self.W_v(values), self.head, True)\n\n        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n        \n        output = self.reshape_tensor(o_reshaped, self.head, False)\n\n        return self.W_o(output)","metadata":{"cell_id":"cc12b62ea2a14ee2854c689e70f73e29","source_hash":"29229744","execution_start":1685381964234,"execution_millis":37,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout","metadata":{"cell_id":"67cbcefc3b144ddabbaac48c132bc052","source_hash":"5b1b7c06","execution_start":1685381970482,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class AddNormalization(Layer):\n    def __init__(self, **kwargs):\n        super(AddNormalization, self).__init__(**kwargs)\n        self.layer_norm = LayerNormalization()\n\n    def call(self, x, sublayer_x):\n        add = x + sublayer_x\n        return self.layer_norm(add)\n\nclass FeedForward(Layer):\n    def __init__(self, d_ff, d_model, **kwargs):\n        super(FeedForward, self).__init__(**kwargs)\n        self.fully_connected1 = Dense(d_ff)\n        self.fully_connected2 = Dense(d_model)\n        self.activation = ReLU()\n\n    def call(self, x):\n        x_fc1 = self.fully_connected1(x)\n        return self.fully_connected2(self.activation(x_fc1))","metadata":{"cell_id":"b481fbf8e1dc465e89da818d4e5e7742","source_hash":"e7481df4","execution_start":1685382004190,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class EncoderLayer(Layer):\n    def __init__(self, sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n        super(EncoderLayer, self).__init__(**kwargs)\n        self.build(input_shape=[None, sequence_length, d_model])\n        self.d_model = d_model\n        self.sequence_length = sequence_length\n        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout1 = Dropout(rate)\n        self.add_norm1 = AddNormalization()\n        self.feed_forward = FeedForward(d_ff, d_model)\n        self.dropout2 = Dropout(rate)\n        self.add_norm2 = AddNormalization()\n\n    def build_graph(self):\n        input_layer = Input(shape=(self.sequence_length, self.d_model))\n        return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n\n    def call(self, x, padding_mask, training):\n        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n\n        multihead_output = self.dropout1(multihead_output, training=training)\n\n        addnorm_output = self.add_norm1(x, multihead_output)\n\n        feedforward_output = self.feed_forward(addnorm_output)\n        feedforward_output = self.dropout2(feedforward_output, training=training)\n\n        return self.add_norm2(addnorm_output, feedforward_output)","metadata":{"cell_id":"2d5142d425e64be78df3d921e07ea991","source_hash":"952339d9","execution_start":1685383917326,"execution_millis":14,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":41},{"cell_type":"code","source":"class Encoder(Layer):\n    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n        self.dropout = Dropout(rate)\n        self.encoder_layer = [EncoderLayer(sequence_length,h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n\n    def call(self, input_sentence, padding_mask, training):\n        pos_encoding_output = self.pos_encoding(input_sentence)\n\n        x = self.dropout(pos_encoding_output, training=training)\n\n        for i, layer in enumerate(self.encoder_layer):\n            x = layer(x, padding_mask, training)\n\n        return x","metadata":{"cell_id":"10c5d3454d9048c59ffd9c56f0f77f5e","source_hash":"de092d46","execution_start":1685383930367,"execution_millis":8,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# from muti_head_attention import MultiHeadAttention\n# from encoder import AddNormalization, FeedForward","metadata":{"cell_id":"55a012f289bc4f5e8151eeccbcbd151d","source_hash":"e7520e14","execution_start":1685382010698,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Implementing the Decoder Layer","metadata":{"cell_id":"468f795f1a6b4243b153cd0598ba9f10","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"class DecoderLayer(Layer):\n    def __init__(self, sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n        super(DecoderLayer, self).__init__(**kwargs)\n        self.build(input_shape=[None, sequence_length, d_model])\n        self.d_model = d_model\n        self.sequence_length = sequence_length\n        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout1 = Dropout(rate)\n        self.add_norm1 = AddNormalization()\n        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout2 = Dropout(rate)\n        self.add_norm2 = AddNormalization()\n        self.feed_forward = FeedForward(d_ff, d_model)\n        self.dropout3 = Dropout(rate)\n        self.add_norm3 = AddNormalization()\n \n    def build_graph(self):\n        input_layer = Input(shape=(self.sequence_length, self.d_model))\n        return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n    \n    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n        multihead_output1 = self.dropout1(multihead_output1, training=training)\n        addnorm_output1 = self.add_norm1(x, multihead_output1)\n\n        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n        multihead_output2 = self.dropout2(multihead_output2, training=training)\n        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n \n        feedforward_output = self.feed_forward(addnorm_output2)\n        feedforward_output = self.dropout3(feedforward_output, training=training)\n \n        return self.add_norm3(addnorm_output2, feedforward_output)","metadata":{"cell_id":"27386320d5c74b3281b391443ae79f7c","source_hash":"9ad6888d","execution_start":1685383945004,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"### Implementing the Decoder","metadata":{"cell_id":"6f8dc628becb4078b9fdf9ca5497d626","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"class Decoder(Layer):\n    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n        self.dropout = Dropout(rate)\n        self.decoder_layer = [DecoderLayer(sequence_length,h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n \n    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n        pos_encoding_output = self.pos_encoding(output_target)\n        x = self.dropout(pos_encoding_output, training=training)\n \n        for i, layer in enumerate(self.decoder_layer):\n            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n \n        return x","metadata":{"cell_id":"8ab6020b2c9c463fa5489178b03148f9","source_hash":"30fafcd0","execution_start":1685383959146,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"### Test","metadata":{"cell_id":"809de431071c4fdb9dd50a243081520e","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"from numpy import random\n \ndec_vocab_size = 20  # Vocabulary size for the decoder\ninput_seq_length = 5  # Maximum length of the input sequence\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nn = 6  # Number of layers in the decoder stack\n \nbatch_size = 64  # Batch size from the training process\ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n \ninput_seq = random.random((batch_size, input_seq_length))\nenc_output = random.random((batch_size, input_seq_length, d_model))\n \ndecoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)","metadata":{"cell_id":"79bd485b1f01463582254dac67b9ce2b","source_hash":"1da710b","execution_start":1685382021490,"execution_millis":195,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(decoder(input_seq, enc_output, None, True))","metadata":{"cell_id":"1206633307a44073b1a1cbe67ca2841e","source_hash":"f9d061fd","execution_start":1685382022765,"execution_millis":1132,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-05-29 17:40:22.741590: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-05-29 17:40:22.741647: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-05-29 17:40:22.741687: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-b5f739dc-f641-4c72-a448-d84edd2bf5bd): /proc/driver/nvidia/version does not exist\n2023-05-29 17:40:22.742387: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\ntf.Tensor(\n[[[ 0.84191734 -1.2020544  -0.1538903  ...  1.0495435   1.664333\n   -0.29789454]\n  [ 0.93142426 -1.1919416  -0.05742599 ...  1.0468749   1.5890642\n   -0.3117879 ]\n  [ 0.91196567 -1.2294681  -0.06891984 ...  1.0676724   1.5354657\n   -0.29635334]\n  [ 0.7958029  -1.2282195  -0.18618879 ...  1.093164    1.5494899\n   -0.26388282]\n  [ 0.70185804 -1.1159515  -0.34444848 ...  1.133866    1.5980263\n   -0.19673692]]\n\n [[ 1.3156121  -1.5436149  -0.22885825 ...  0.8627676   0.94702995\n   -0.44152072]\n  [ 1.4211452  -1.5446004  -0.16143402 ...  0.8653047   0.8989175\n   -0.4768331 ]\n  [ 1.4296132  -1.5954934  -0.18389873 ...  0.88599086  0.86401314\n   -0.5050909 ]\n  [ 1.3541392  -1.5946798  -0.29129195 ...  0.92051905  0.87903035\n   -0.4901426 ]\n  [ 1.2796001  -1.4954208  -0.42180437 ...  0.9437834   0.9293653\n   -0.4374321 ]]\n\n [[ 0.7570926  -1.5393484  -0.4492812  ...  1.2732064   1.259069\n   -0.22207068]\n  [ 0.8500167  -1.5469003  -0.34588334 ...  1.2578058   1.2057552\n   -0.23759598]\n  [ 0.838369   -1.5927019  -0.3381339  ...  1.260752    1.1863061\n   -0.25646076]\n  [ 0.7337623  -1.5902584  -0.45394608 ...  1.2720752   1.1990556\n   -0.23903118]\n  [ 0.6584741  -1.4705116  -0.6036017  ...  1.2943934   1.22877\n   -0.2000055 ]]\n\n ...\n\n [[ 0.8457851  -1.3215921  -0.3272315  ...  1.3442421   1.1359328\n   -0.11508673]\n  [ 0.923225   -1.3323712  -0.21702968 ...  1.3322148   1.0865357\n   -0.10441758]\n  [ 0.90207505 -1.3632971  -0.2020964  ...  1.3450894   1.0591668\n   -0.08742072]\n  [ 0.80768627 -1.3181374  -0.29415116 ...  1.3731211   1.0411351\n   -0.06810641]\n  [ 0.7406552  -1.1927245  -0.42556408 ...  1.4144424   1.0388981\n   -0.01972626]]\n\n [[ 0.9595775  -1.645372   -0.2720369  ...  1.0859741   1.3435382\n   -0.5126426 ]\n  [ 1.0464169  -1.6300654  -0.15959448 ...  1.0662335   1.2633315\n   -0.55159944]\n  [ 1.0329273  -1.6419866  -0.1391747  ...  1.0827407   1.2213707\n   -0.5765554 ]\n  [ 0.9400829  -1.6221098  -0.24445705 ...  1.1157608   1.2209905\n   -0.53951854]\n  [ 0.8700494  -1.5171983  -0.39167058 ...  1.1701595   1.2477777\n   -0.46441123]]\n\n [[ 1.0799161  -1.1891358  -0.24216615 ...  0.8725154   1.1090702\n   -0.2679454 ]\n  [ 1.1811842  -1.1787858  -0.14811423 ...  0.8646297   1.0643319\n   -0.28612596]\n  [ 1.1739715  -1.2001047  -0.13378845 ...  0.88393956  1.037373\n   -0.30377266]\n  [ 1.0684615  -1.1784042  -0.21920748 ...  0.93487716  1.0396671\n   -0.28144   ]\n  [ 0.9897634  -1.0722362  -0.37002194 ...  0.9808384   1.0533075\n   -0.22714312]]], shape=(64, 5, 512), dtype=float32)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# from encoder import Encoder\n# from decoder import Decoder\nfrom tensorflow import math, cast, float32, linalg, ones, maximum, newaxis\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense, Input","metadata":{"cell_id":"4b27d8cbc10446f789eb8005bdd804e8","source_hash":"e62a3ecb","execution_start":1685383992220,"execution_millis":13,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":47},{"cell_type":"code","source":"class TransformerModel(Model):\n    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n        super(TransformerModel, self).__init__(**kwargs)\n\n        #setup the encoder\n        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        #setup decoder\n        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        #final dense layer\n        self.model_last_layer = Dense(dec_vocab_size)\n\n    def padding_mask(self, input):\n        mask = math.equal(input, 0)\n        mask = cast(mask, float32)\n\n        return mask[:, newaxis, newaxis, :]\n\n    def lookahed_mask(self, shape):\n        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n        return mask\n\n    def call(self, encoder_input, decoder_input, training):\n        enc_padding_mask = self.padding_mask(encoder_input)\n\n        dec_in_padding_mask = self.padding_mask(decoder_input)\n        dec_in_lookahead_mask = self.lookahed_mask(decoder_input.shape[1])\n        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n\n        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n\n        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n\n        model_output = self.model_last_layer(decoder_output)\n\n        return model_output        ","metadata":{"cell_id":"aac44a67feb645809e913d5595374bcb","source_hash":"5a4c5a52","execution_start":1685383994511,"execution_millis":9,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":48},{"cell_type":"code","source":"h = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nn = 6  # Number of layers in the encoder stack\n \ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers","metadata":{"cell_id":"dc4936c36ba44c24bb77b861b1f30771","source_hash":"5f256eaf","execution_start":1685382031458,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":14},{"cell_type":"code","source":"enc_vocab_size = 20 # Vocabulary size for the encoder\ndec_vocab_size = 20 # Vocabulary size for the decoder\n \nenc_seq_length = 5  # Maximum length of the input sequence\ndec_seq_length = 5  # Maximum length of the target sequence","metadata":{"cell_id":"0f4860f5cdd849b38a4eae89b6a942a0","source_hash":"4149a81f","execution_start":1685382032751,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# from model import TransformerModel\n \ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)","metadata":{"cell_id":"aa9e27b7cf2746b6aac144e2df854b5e","source_hash":"e3825f48","execution_start":1685383970990,"execution_millis":11366,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":46},{"cell_type":"code","source":"encoder = EncoderLayer(enc_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\nencoder.build_graph().summary()\n \ndecoder = DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\ndecoder.build_graph().summary()","metadata":{"cell_id":"bbda8bb7afee4168b66ee62e2fadbdd7","source_hash":"16444627","execution_start":1685383999390,"execution_millis":782,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 6, 512)]     0           []                               \n                                                                                                  \n multi_head_attention_34 (Multi  (None, 6, 512)      131776      ['input_1[0][0]',                \n HeadAttention)                                                   'input_1[0][0]',                \n                                                                  'input_1[0][0]']                \n                                                                                                  \n dropout_58 (Dropout)           (None, 6, 512)       0           ['multi_head_attention_34[0][0]']\n                                                                                                  \n add_normalization_55 (AddNorma  (None, 6, 512)      1024        ['input_1[0][0]',                \n lization)                                                        'dropout_58[0][0]']             \n                                                                                                  \n feed_forward_21 (FeedForward)  (None, 6, 512)       2099712     ['add_normalization_55[0][0]']   \n                                                                                                  \n dropout_59 (Dropout)           (None, 6, 512)       0           ['feed_forward_21[0][0]']        \n                                                                                                  \n add_normalization_56 (AddNorma  (None, 6, 512)      1024        ['add_normalization_55[0][0]',   \n lization)                                                        'dropout_59[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 2,233,536\nTrainable params: 2,233,536\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"},{"output_type":"error","ename":"TypeError","evalue":"call() missing 2 required positional arguments: 'padding_mask' and 'training'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [49], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m encoder\u001b[38;5;241m.\u001b[39mbuild_graph()\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m      4\u001b[0m decoder \u001b[38;5;241m=\u001b[39m DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msummary()\n","Cell \u001b[0;32mIn [43], line 19\u001b[0m, in \u001b[0;36mDecoderLayer.build_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_graph\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     18\u001b[0m     input_layer \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model))\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Model(inputs\u001b[38;5;241m=\u001b[39m[input_layer], outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n","\u001b[0;31mTypeError\u001b[0m: call() missing 2 required positional arguments: 'padding_mask' and 'training'"]}],"execution_count":49},{"cell_type":"markdown","source":"### Transformer model","metadata":{"cell_id":"deb5c9fac977437b8c7f939618415bb2","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"# from encoder import Encoder, EncoderLayer\n# from decoder import Decoder, DecoderLayer\nfrom tensorflow import math, cast, float32, linalg, ones, maximum, newaxis\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense\n \n \nclass TransformerModel(Model):\n    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n        super(TransformerModel, self).__init__(**kwargs)\n\n        self.encoder_layer = EncoderLayer(enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, rate)\n        self.decoder_layer = DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, rate)\n \n        # Set up the encoder\n        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n \n        # Set up the decoder\n        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n \n        # Define the final dense layer\n        self.model_last_layer = Dense(dec_vocab_size)\n \n    def padding_mask(self, input):\n        # Create mask which marks the zero padding values in the input by a 1.0\n        mask = math.equal(input, 0)\n        mask = cast(mask, float32)\n \n        # The shape of the mask should be broadcastable to the shape\n        # of the attention weights that it will be masking later on\n        return mask[:, newaxis, newaxis, :]\n \n    def lookahead_mask(self, shape):\n        # Mask out future entries by marking them with a 1.0\n        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n \n        return mask\n    \n    def model_summary(self):\n        # Show model summary\n        encoder_summary = self.encoder_layer.build_graph().summary()\n        decoder_summary = self.decoder_layer.build_graph().summary()\n        return encoder_summary, decoder_summary\n \n    def call(self, encoder_input, decoder_input, training):\n \n        # Create padding mask to mask the encoder inputs and the encoder outputs in the decoder\n        enc_padding_mask = self.padding_mask(encoder_input)\n \n        # Create and combine padding and look-ahead masks to be fed into the decoder\n        dec_in_padding_mask = self.padding_mask(decoder_input)\n        dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n \n        # Feed the input into the encoder\n        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n \n        # Feed the encoder output into the decoder\n        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n \n        # Pass the decoder output through a final dense layer\n        model_output = self.model_last_layer(decoder_output)\n \n        return model_output\n    ","metadata":{"cell_id":"79bbe20c3d28436cb0227b2dd1f4fd88","source_hash":"993d4494","execution_start":1685384005962,"execution_millis":17,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":50},{"cell_type":"code","source":"from pickle import load\nfrom numpy.random import shuffle\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\nfrom tensorflow  import convert_to_tensor, int64","metadata":{"cell_id":"e091f74e80e1456fbf47245a224f183e","source_hash":"fe21f9e7","execution_start":1685384008281,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":51},{"cell_type":"code","source":"class PrepareDataset:\n    def __init__(self, **kawrgs):\n        super(PrepareDataset, self).__init__(**kawrgs)\n        self.n_sentences = 10000\n        self.train_split = 0.9\n\n    def create_tokenizer(self, dataset):\n        tokenizer = Tokenizer()\n        tokenizer.fit_on_texts(dataset)\n        return tokenizer\n\n    def find_seq_length(self, dataset):\n        return max(len(seq.split()) for seq in dataset)\n\n    def find_vocab_size(self, tokenizer, dataset):\n        tokenizer.fit_on_texts(dataset)\n        return len(tokenizer.word_index) + 1\n\n    def __call__(self, filename, **kwargs):\n        clean_dataset = load(open(filename, 'rb'))\n\n        dataset = clean_dataset[:self.n_sentences, :]\n\n        for i in range(dataset[:, 0].size):\n            dataset[i, 0] = \"<START>\" + dataset[i, 0] + \" <EOS>\"\n            dataset[i, 1] = \"<START>\" + dataset[i, 1] + \" <EOS>\"\n\n        shuffle(dataset)\n\n        train = dataset[:int(self.n_sentences * self.train_split)]\n        \n        enc_tokenizer = self.create_tokenizer(train[:, 0])\n        enc_seq_length = self.find_seq_length(train[:, 0])\n        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n\n\n        trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n        trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n        trainX = convert_to_tensor(trainX, dtype=int64)\n\n        dec_tokenizer = self.create_tokenizer(train[:, 1])\n        dec_seq_length = self.find_seq_length(train[:, 1])\n        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n\n        trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n        trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n        trainY = convert_to_tensor(trainY, dtype=int64)\n        \n        return trainX, trainY, train, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size","metadata":{"cell_id":"4c7a74a6106c49f5bbac7512916b3263","source_hash":"d9090ec4","execution_start":1685383617200,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Prepare the training data\ndataset = PrepareDataset()\ntrainX, trainY, train_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n \nprint(train_orig[0, 0], '\\n', trainX[0, :])","metadata":{"cell_id":"4c036a912c08457b9dfa3f9a936d98fd","source_hash":"f0ea127a","execution_start":1685383618706,"execution_millis":1089,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"<START>do you deny it <EOS> \n tf.Tensor([   1   12    5 1440    6    2], shape=(6,), dtype=int64)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"print('Encoder sequence length:', enc_seq_length)","metadata":{"cell_id":"79d451787533405e90301248685a4a6a","source_hash":"d40cf17d","execution_start":1685383691800,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Encoder sequence length: 6\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"print(train_orig[0, 1], '\\n', trainY[0, :])","metadata":{"cell_id":"ebe371f736314c4b963424acc26af121","source_hash":"e3cc2302","execution_start":1685383711736,"execution_millis":16,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"<START>leugnest du <EOS> \n tf.Tensor([   1 1867   12    2    0    0    0    0    0    0    0], shape=(11,), dtype=int64)\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"print('Decoder sequence length:', dec_seq_length)","metadata":{"cell_id":"6efa0fa1d9cd49128769d7b4f942880b","source_hash":"dd2bba8c","execution_start":1685383723367,"execution_millis":10,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Decoder sequence length: 11\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import LearningRateSchedule\nfrom tensorflow.keras.metrics import Mean\nfrom tensorflow import data, train, math, reduce_sum, cast, equal, argmax, float32, GradientTape, TensorSpec, function, int64\nfrom keras.losses import sparse_categorical_crossentropy\nfrom time import time\n \n \n# Define the model parameters\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of model layers' outputs\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nn = 6  # Number of layers in the encoder stack\n \n# Define the training parameters\nepochs = 2\nbatch_size = 64\nbeta_1 = 0.9\nbeta_2 = 0.98\nepsilon = 1e-9\ndropout_rate = 0.1\n \n \n# Implementing a learning rate scheduler\nclass LRScheduler(LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n        super(LRScheduler, self).__init__(**kwargs)\n \n        self.d_model = cast(d_model, float32)\n        self.warmup_steps = warmup_steps\n \n    def __call__(self, step_num):\n \n        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n        arg1 = step_num ** -0.5\n        arg2 = step_num * (self.warmup_steps ** -1.5)\n \n        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n \n \n# Instantiate an Adam optimizer\noptimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n \n# Prepare the training and test splits of the dataset\ndataset = PrepareDataset()\ntrainX, trainY, train_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n \n# Prepare the dataset batches\ntrain_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\ntrain_dataset = train_dataset.batch(batch_size)\n \n# Create model\ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n \n \n# Defining the loss function\ndef loss_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of loss\n    padding_mask = math.logical_not(equal(target, 0))\n    padding_mask = cast(padding_mask, float32)\n \n    # Compute a sparse categorical cross-entropy loss on the unmasked values\n    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n \n    # Compute the mean loss over the unmasked values\n    return reduce_sum(loss) / reduce_sum(padding_mask)\n \n \n# Defining the accuracy function\ndef accuracy_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of accuracy\n    padding_mask = math.logical_not(equal(target, 0))\n \n    # Find equal prediction and target values, and apply the padding mask\n    accuracy = equal(target, argmax(prediction, axis=2))\n    accuracy = math.logical_and(padding_mask, accuracy)\n \n    # Cast the True/False values to 32-bit-precision floating-point numbers\n    padding_mask = cast(padding_mask, float32)\n    accuracy = cast(accuracy, float32)\n \n    # Compute the mean accuracy over the unmasked values\n    return reduce_sum(accuracy) / reduce_sum(padding_mask)\n \n \n# Include metrics monitoring\ntrain_loss = Mean(name='train_loss')\ntrain_accuracy = Mean(name='train_accuracy')\n \n# Create a checkpoint object and manager to manage multiple checkpoints\nckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\nckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)\n \n# Speeding up the training process\n@function\ndef train_step(encoder_input, decoder_input, decoder_output):\n    with GradientTape() as tape:\n \n        # Run the forward pass of the model to generate a prediction\n        prediction = training_model(encoder_input, decoder_input, training=True)\n \n        # Compute the training loss\n        loss = loss_fcn(decoder_output, prediction)\n \n        # Compute the training accuracy\n        accuracy = accuracy_fcn(decoder_output, prediction)\n \n    # Retrieve gradients of the trainable variables with respect to the training loss\n    gradients = tape.gradient(loss, training_model.trainable_weights)\n \n    # Update the values of the trainable variables by gradient descent\n    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n \n    train_loss(loss)\n    train_accuracy(accuracy)\n \n \nfor epoch in range(epochs):\n \n    train_loss.reset_states()\n    train_accuracy.reset_states()\n \n    print(\"\\nStart of epoch %d\" % (epoch + 1))\n \n    start_time = time()\n \n    # Iterate over the dataset batches\n    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n \n        # Define the encoder and decoder inputs, and the decoder output\n        encoder_input = train_batchX[:, 1:]\n        decoder_input = train_batchY[:, :-1]\n        decoder_output = train_batchY[:, 1:]\n \n        train_step(encoder_input, decoder_input, decoder_output)\n \n        if step % 50 == 0:\n            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n            # print(\"Samples so far: %s\" % ((step + 1) * batch_size))\n \n    # Print epoch number and loss value at the end of every epoch\n    print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result()))\n \n    # Save a checkpoint after every five epochs\n    if (epoch + 1) % 5 == 0:\n        save_path = ckpt_manager.save()\n        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))\n \nprint(\"Total time taken: %.2fs\" % (time() - start_time))","metadata":{"cell_id":"db3465be6fa3423f8205317d5be9fe08","source_hash":"bbc494c1","execution_start":1685384011947,"execution_millis":801130,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"\nStart of epoch 1\nEpoch 1 Step 0 Loss 8.4071 Accuracy 0.0000\nEpoch 1 Step 50 Loss 7.6891 Accuracy 0.1266\nEpoch 1 Step 100 Loss 7.0589 Accuracy 0.1728\nEpoch 1: Training Loss 6.7279, Training Accuracy 0.1943\n\nStart of epoch 2\nEpoch 2 Step 0 Loss 5.6478 Accuracy 0.2667\nEpoch 2 Step 50 Loss 5.4291 Accuracy 0.2724\nEpoch 2 Step 100 Loss 5.2634 Accuracy 0.2814\nEpoch 2: Training Loss 5.1535, Training Accuracy 0.2873\nTotal time taken: 385.55s\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"","metadata":{"cell_id":"c6b0f915742e4dc88412ad154056a8a3","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b5f739dc-f641-4c72-a448-d84edd2bf5bd' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"1b6b77d280cd46d0b370f127bceb6b04","deepnote_execution_queue":[]}}