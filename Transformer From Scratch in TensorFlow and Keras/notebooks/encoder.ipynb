{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"b2e3114aa7984405a2c077b70f7fa880","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1192,"execution_start":1684262860948,"source_hash":"c3f60b9a"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","from tensorflow import convert_to_tensor, string\n","from tensorflow.keras.layers import Embedding, Layer, LayerNormalization, Dense, ReLU, Dropout\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","from tensorflow import matmul, reshape, shape, transpose, cast, float32\n","from keras.backend import softmax"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","class PositionEmbeddingFixedWeights(Layer):\n","    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n","        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n","        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n","        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n","        self.word_embedding_layer = Embedding(\n","            input_dim=vocab_size, output_dim=output_dim,\n","            weights=[word_embedding_matrix],\n","            trainable=False\n","        )\n","        self.position_embedding_layer = Embedding(\n","            input_dim=sequence_length, output_dim=output_dim,\n","            weights=[position_embedding_matrix],\n","            trainable=False\n","        )\n","             \n","    def get_position_encoding(self, seq_len, d, n=10000):\n","        P = np.zeros((seq_len, d))\n","        for k in range(seq_len):\n","            for i in np.arange(int(d/2)):\n","                denominator = np.power(n, 2*i/d)\n","                P[k, 2*i] = np.sin(k/denominator)\n","                P[k, 2*i+1] = np.cos(k/denominator)\n","        return P\n"," \n"," \n","    def call(self, inputs):        \n","        position_indices = tf.range(tf.shape(inputs)[-1])\n","        embedded_words = self.word_embedding_layer(inputs)\n","        embedded_indices = self.position_embedding_layer(position_indices)\n","        return embedded_words + embedded_indices"]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"f992c2feae1342cc91d83e3d1a79cfea","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":29,"execution_start":1684262877345,"source_hash":"4a52b50f"},"outputs":[],"source":["class DotProductAttention(Layer):\n","    def __init__(self, **kwargs):\n","        super(DotProductAttention, self).__init__(**kwargs)\n","\n","    def call(self, queries, keys, values, d_k, mask=None):\n","        scores = matmul(queries, keys, transpose_b=True) / tf.math.sqrt(cast(d_k, float32))\n","        \n","        if mask is not None:\n","            score += -1e9 * mask\n","\n","        weight = softmax(scores)\n","\n","        return matmul(weight, values)\n","\n","class MultiHeadAttention(Layer):\n","    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n","        super(MultiHeadAttention, self).__init__(**kwargs)\n","        self.attention = DotProductAttention()\n","        self.head = h\n","        self.d_k = d_k\n","        self.d_v = d_v\n","        self.d_model = d_model\n","        self.W_q = Dense(d_k)\n","        self.W_k = Dense(d_k)\n","        self.W_v = Dense(d_v)\n","        self.W_o = Dense(d_model)\n","\n","    def reshape_tensor(self, x, head, flag):\n","        if flag:\n","            x = reshape(x, shape=(shape(x)[0], shape(x)[1], head, -1))\n","            x = transpose(x, perm=(0, 2, 1, 3))\n","        else:\n","            x = transpose(x, perm=(0, 2, 1, 3))\n","            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n","        return x\n","\n","    def call(self, queries, keys, values, mask=None):\n","        q_reshaped = self.reshape_tensor(self.W_q(queries), self.head, True)\n","        k_reshaped = self.reshape_tensor(self.W_k(keys), self.head, True)\n","        v_reshaped = self.reshape_tensor(self.W_v(values), self.head, True)\n","\n","        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n","        \n","        output = self.reshape_tensor(o_reshaped, self.head, False)\n","\n","        return self.W_o(output)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"f4c5c4d48a8d4566bb2671090cd69fd8","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Implementing the Add & Norm Layer"]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"f9df6595be4e483aabead6e42108238b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":15,"execution_start":1684263444506,"source_hash":"a85e47a1"},"outputs":[],"source":["class AddNormalization(Layer):\n","    def __init__(self, **kwargs):\n","        super(AddNormalization, self).__init__(**kwargs)\n","        self.layer_norm = LayerNormalization()\n","\n","    def call(self, x, sublayer_x):\n","        add = x + sublayer_x\n","        return self.layer_norm(add)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"7ff0f32a8c174b0080823a12a9234408","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Implementing the Feed-Forward Layer"]},{"cell_type":"code","execution_count":19,"metadata":{"cell_id":"d9242c2ec51e47f7892cbef9d0401f56","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1684265245886,"source_hash":"bda47d3e"},"outputs":[],"source":["class FeedForward(Layer):\n","    def __init__(self, d_ff, d_model, **kwargs):\n","        super(FeedForward, self).__init__(**kwargs)\n","        self.fully_connected1 = Dense(d_ff)\n","        self.fully_connected2 = Dense(d_model)\n","        self.activation = ReLU()\n","\n","    def call(self, x):\n","        x_fc1 = self.fully_connected1(x)\n","        return self.fully_connected2(self.activation(x_fc1))"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"616032ddb23e46cea81a0858dea0e2b0","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Implementing the Encoder Layer"]},{"cell_type":"code","execution_count":28,"metadata":{"cell_id":"3b5807e1303e47d68ace770f178c1460","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1684265548500,"source_hash":"48d4481b"},"outputs":[],"source":["class EncoderLayer(Layer):\n","    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n","        super(EncoderLayer, self).__init__(**kwargs)\n","        self.muti_head_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n","        self.dropout1 = Dropout(rate)\n","        self.add_norm1 = AddNormalization()\n","        self.feed_forward = FeedForward(d_ff, d_model)\n","        self.dropout2 = Dropout(rate)\n","        self.add_norm2 = AddNormalization()\n","\n","    def call(self, x, padding_mask, training):\n","        multihead_output = self.muti_head_attention(x, x, x, padding_mask)\n","        multihead_output = self.dropout1(multihead_output, training=training)\n","        addnorm_output = self.add_norm1(x, multihead_output)\n","        feedforward_output = self.feed_forward(addnorm_output)\n","        feedforward_output = self.dropout2(feedforward_output, training=training)\n","        return self.add_norm2(addnorm_output, feedforward_output)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"ab6e4525677f4b71a169faf1862d2151","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Implementing the Encoder"]},{"cell_type":"code","execution_count":24,"metadata":{"cell_id":"56bb643c52a34a979455b199886db874","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":60,"execution_start":1684265502615,"source_hash":"a11265f4"},"outputs":[],"source":["class Encoder(Layer):\n","    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n","        super(Encoder, self).__init__(**kwargs)\n","        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n","        self.dropout = Dropout(rate)\n","        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n","\n","    def call(self, input_sentence, padding_mask, training):\n","        pos_encoding_output = self.pos_encoding(input_sentence)\n","        x = self.dropout(pos_encoding_output, training=training)\n","\n","        for i, layer in enumerate(self.encoder_layer):\n","            x = layer(x, padding_mask, training)\n","\n","        return x"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"b444c293b4e74729886336a2d8419204","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Test"]},{"cell_type":"code","execution_count":29,"metadata":{"cell_id":"ecda02d096194f3b834225b7cb5dcb6d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1029,"execution_start":1684265554470,"source_hash":"e9e3984"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[[[-1.45917624e-01 -9.15344059e-01  1.45258874e-01 ...  2.06214339e-01\n","   -1.76602054e+00  2.61561185e-01]\n","  [ 3.41569073e-02 -6.05697691e-01 -4.38042223e-01 ...  3.00020933e-01\n","   -1.73444092e+00  2.31028005e-01]\n","  [ 4.25433069e-01 -1.21905255e+00  2.99051464e-01 ... -2.81086773e-01\n","   -1.62082088e+00  3.20402622e-01]\n","  [ 1.99012801e-01 -1.45455492e+00  3.18659157e-01 ...  1.19393110e-01\n","   -1.69570315e+00  2.39699095e-01]\n","  [-8.49100649e-02 -1.70686495e+00  3.44290167e-01 ...  1.34133554e+00\n","   -1.74305832e+00 -3.48057628e-01]]\n","\n"," [[ 6.91816151e-01 -4.11242127e-01  7.75505662e-01 ...  6.14771128e-01\n","   -1.29956734e+00 -2.73143202e-01]\n","  [ 3.14976186e-01  2.32824728e-01  1.57901540e-01 ... -1.40927851e-01\n","   -6.43691003e-01 -1.75628498e-01]\n","  [ 8.16481352e-01 -1.51634872e+00  4.83508129e-03 ...  7.69579351e-01\n","   -6.14210725e-01  8.06200653e-02]\n","  [ 4.98605371e-01 -7.62940168e-01 -9.90712196e-02 ...  1.08738191e-01\n","   -6.31547511e-01  3.07440966e-01]\n","  [-3.76309529e-02 -3.87747198e-01 -1.07937425e-01 ...  4.67787206e-01\n","   -8.56293976e-01 -3.39751691e-01]]\n","\n"," [[ 1.10851312e+00 -1.15290269e-01  5.78692555e-01 ...  8.32432434e-02\n","   -1.12431359e+00  4.21984285e-01]\n","  [ 1.57252228e+00 -1.00631046e+00  7.24592149e-01 ...  1.71710968e-01\n","   -1.39441466e+00  3.23429078e-01]\n","  [ 5.75247169e-01 -7.94324756e-01  9.51232672e-01 ... -6.31730945e-05\n","   -7.43506730e-01  4.31691319e-01]\n","  [ 4.40966994e-01 -1.32351732e+00  7.43558764e-01 ... -9.25953150e-01\n","   -1.19611096e+00 -3.47423583e-01]\n","  [ 1.02400124e+00 -5.32325566e-01  6.46120012e-01 ...  2.22186416e-01\n","   -1.33483875e+00  1.16825700e-01]]\n","\n"," ...\n","\n"," [[ 2.48767823e-01 -5.69494307e-01  3.62085700e-01 ...  1.81434155e-01\n","   -1.09556425e+00  4.56618518e-03]\n","  [ 8.25965285e-01 -1.28571248e+00  6.24059200e-01 ...  1.79107338e-01\n","   -8.15331399e-01  1.76434159e-01]\n","  [ 5.01228452e-01 -8.56967449e-01  5.63016295e-01 ...  1.20458469e-01\n","   -1.08412254e+00  1.37278110e-01]\n","  [-8.57183244e-03 -2.94842422e-01  1.00359559e+00 ...  2.11106077e-01\n","   -1.06472254e+00  4.49708030e-02]\n","  [ 2.02894017e-01 -1.35858381e+00  8.17070186e-01 ... -3.50592643e-01\n","   -7.91304529e-01  9.60433111e-02]]\n","\n"," [[ 1.70494545e+00 -1.08770025e+00  6.75447345e-01 ... -2.03654706e-01\n","   -1.17161655e+00  1.59209535e-01]\n","  [ 1.27558970e+00 -1.02415872e+00  8.20741534e-01 ... -2.39566579e-01\n","   -1.03421926e+00  1.54786989e-01]\n","  [ 9.02758300e-01 -1.72025061e+00  7.64663398e-01 ... -6.25123084e-01\n","   -8.89661551e-01  7.01586723e-01]\n","  [ 9.32236612e-01 -1.26774144e+00  7.47065246e-01 ... -3.63653332e-01\n","   -8.90539348e-01  5.82894325e-01]\n","  [ 4.22770560e-01 -6.81039155e-01  5.71410060e-02 ... -2.54617661e-01\n","   -7.02492833e-01  4.34921741e-01]]\n","\n"," [[ 6.17305696e-01 -7.53783047e-01  7.87375212e-01 ... -3.21615279e-01\n","   -1.25545537e+00  4.36716616e-01]\n","  [ 7.78083444e-01 -6.16765618e-01  2.26791263e-01 ... -3.97561759e-01\n","   -1.67787409e+00 -8.64611566e-02]\n","  [ 1.21600306e+00 -2.47268945e-01  1.02781057e-01 ...  6.75929368e-01\n","   -9.06359017e-01  2.32233748e-01]\n","  [ 4.35616374e-01 -8.64731312e-01  3.70041966e-01 ...  1.90373972e-01\n","   -1.37905335e+00  3.24342877e-01]\n","  [ 1.07401818e-01 -1.43658566e+00  6.87449723e-02 ... -3.10269948e-02\n","   -1.25186765e+00  1.77945077e-01]]], shape=(64, 5, 512), dtype=float32)\n"]}],"source":["from numpy import random\n","\n","enc_vocab_size = 20 # Vocabulary size for the encoder\n","input_seq_length = 5  # Maximum length of the input sequence\n","h = 8  # Number of self-attention heads\n","d_k = 64  # Dimensionality of the linearly projected queries and keys\n","d_v = 64  # Dimensionality of the linearly projected values\n","d_ff = 2048  # Dimensionality of the inner fully connected layer\n","d_model = 512  # Dimensionality of the model sub-layers' outputs\n","n = 6  # Number of layers in the encoder stack\n"," \n","batch_size = 64  # Batch size from the training process\n","dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n"," \n","input_seq = random.random((batch_size, input_seq_length))\n"," \n","encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n","print(encoder(input_seq, None, True))"]},{"attachments":{},"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b5f739dc-f641-4c72-a448-d84edd2bf5bd' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"ef9076edd2444ff0b43998da0f2fe753","language_info":{"name":"python"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
