{"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom tensorflow import convert_to_tensor, string\nfrom tensorflow.keras.layers import Embedding, Layer\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\n\nclass PositionEmbeddingFixedWeights(Layer):\n    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n        self.word_embedding_layer = Embedding(\n            input_dim=vocab_size, output_dim=output_dim,\n            weights=[word_embedding_matrix],\n            trainable=False\n        )\n        self.position_embedding_layer = Embedding(\n            input_dim=sequence_length, output_dim=output_dim,\n            weights=[position_embedding_matrix],\n            trainable=False\n        )\n             \n    def get_position_encoding(self, seq_len, d, n=10000):\n        P = np.zeros((seq_len, d))\n        for k in range(seq_len):\n            for i in np.arange(int(d/2)):\n                denominator = np.power(n, 2*i/d)\n                P[k, 2*i] = np.sin(k/denominator)\n                P[k, 2*i+1] = np.cos(k/denominator)\n        return P\n \n \n    def call(self, inputs):        \n        position_indices = tf.range(tf.shape(inputs)[-1])\n        embedded_words = self.word_embedding_layer(inputs)\n        embedded_indices = self.position_embedding_layer(position_indices)\n        return embedded_words + embedded_indices","metadata":{"cell_id":"9a2bed9716b4433d93ca8f082cf820d9","source_hash":"c3f60b9a","execution_start":1684775730605,"execution_millis":6087,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-05-22 17:15:30.615436: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-22 17:15:30.942488: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-05-22 17:15:30.942522: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-05-22 17:15:31.006419: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-05-22 17:15:32.540916: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-05-22 17:15:32.541001: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-05-22 17:15:32.541010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import matmul, reshape, shape, transpose, cast, float32\nfrom tensorflow.keras.layers import Dense, Layer\nfrom keras.backend import softmax\n\nclass DotProductAttention(Layer):\n    def __init__(self, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n\n    def call(self, queries, keys, values, d_k, mask=None):\n        scores = matmul(queries, keys, transpose_b=True) / tf.math.sqrt(cast(d_k, float32))\n        \n        if mask is not None:\n            scores += -1e9 * mask\n\n        weight = softmax(scores)\n\n        return matmul(weight, values)\n\nclass MultiHeadAttention(Layer):\n    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.attention = DotProductAttention()\n        self.head = h\n        self.d_k = d_k\n        self.d_v = d_v\n        self.d_model = d_model\n        self.W_q = Dense(d_k)\n        self.W_k = Dense(d_k)\n        self.W_v = Dense(d_v)\n        self.W_o = Dense(d_model)\n\n    def reshape_tensor(self, x, head, flag):\n        if flag:\n            x = reshape(x, shape=(shape(x)[0], shape(x)[1], head, -1))\n            x = transpose(x, perm=(0, 2, 1, 3))\n        else:\n            x = transpose(x, perm=(0, 2, 1, 3))\n            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n        return x\n\n    def call(self, queries, keys, values, mask=None):\n        q_reshaped = self.reshape_tensor(self.W_q(queries), self.head, True)\n        k_reshaped = self.reshape_tensor(self.W_k(keys), self.head, True)\n        v_reshaped = self.reshape_tensor(self.W_v(values), self.head, True)\n\n        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n        \n        output = self.reshape_tensor(o_reshaped, self.head, False)\n\n        return self.W_o(output)","metadata":{"cell_id":"69cb748891264f17a73f2469168978c2","source_hash":"29229744","execution_start":1684775736708,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout","metadata":{"cell_id":"5941d9638bf2403b801dc426a163ac6f","source_hash":"5b1b7c06","execution_start":1684775740392,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class AddNormalization(Layer):\n    def __init__(self, **kwargs):\n        super(AddNormalization, self).__init__(**kwargs)\n        self.layer_norm = LayerNormalization()\n\n    def call(self, x, sublayer_x):\n        add = x + sublayer_x\n        return self.layer_norm(add)\n\nclass FeedForward(Layer):\n    def __init__(self, d_ff, d_model, **kwargs):\n        super(FeedForward, self).__init__(**kwargs)\n        self.fully_connected1 = Dense(d_ff)\n        self.fully_connected2 = Dense(d_model)\n        self.activation = ReLU()\n\n    def call(self, x):\n        x_fc1 = self.fully_connected1(x)\n        return self.fully_connected2(self.activation(x_fc1))","metadata":{"cell_id":"4a5e86693a4f4acdbdcf02248c49078a","source_hash":"e7481df4","execution_start":1684775741896,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class EncoderLayer(Layer):\n    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n        super(EncoderLayer, self).__init__(**kwargs)\n        self.muti_head_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout1 = Dropout(rate)\n        self.add_norm1 = AddNormalization()\n        self.feed_forward = FeedForward(d_ff, d_model)\n        self.dropout2 = Dropout(rate)\n        self.add_norm2 = AddNormalization()\n\n    def call(self, x, padding_mask, training):\n        multihead_output = self.muti_head_attention(x, x, x, padding_mask)\n        multihead_output = self.dropout1(multihead_output, training=training)\n        addnorm_output = self.add_norm1(x, multihead_output)\n        feedforward_output = self.feed_forward(addnorm_output)\n        feedforward_output = self.dropout2(feedforward_output, training=training)\n        return self.add_norm2(addnorm_output, feedforward_output)","metadata":{"cell_id":"f39e6bd0b8c0416ba8257485478c9029","source_hash":"48d4481b","execution_start":1684775743675,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Encoder(Layer):\n    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n        self.dropout = Dropout(rate)\n        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n\n    def build_graph(self):\n        input_layer = Input(shape=(self.sequence_length, self.d_model))\n        return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n        \n    def call(self, input_sentence, padding_mask, training):\n        pos_encoding_output = self.pos_encoding(input_sentence)\n        x = self.dropout(pos_encoding_output, training=training)\n\n        for i, layer in enumerate(self.encoder_layer):\n            x = layer(x, padding_mask, training)\n\n        return x","metadata":{"cell_id":"470badd6c7c84c62b8ad4e013d38444b","source_hash":"430e4b7a","execution_start":1684777807724,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# from muti_head_attention import MultiHeadAttention\n# from encoder import AddNormalization, FeedForward","metadata":{"cell_id":"5f9abe204bee42e79bf841328c378580","source_hash":"e7520e14","execution_start":1684777641160,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### Implementing the Decoder Layer","metadata":{"cell_id":"cfe1232b73a34da3abd4a8257f6e23e3","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"class DecoderLayer(Layer):\n    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n        super(DecoderLayer, self).__init__(**kwargs)\n        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout1 = Dropout(rate)\n        self.add_norm1 = AddNormalization()\n        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout2 = Dropout(rate)\n        self.add_norm2 = AddNormalization()\n        self.feed_forward = FeedForward(d_ff, d_model)\n        self.dropout3 = Dropout(rate)\n        self.add_norm3 = AddNormalization()\n\n    def build_graph(self):\n        input_layer = Input(shape=(self.sequence_length, self.d_model))\n        return Model(inputs=[input_layer], outputs=self.call(input_layer, input_layer, None, None, True))\n\n    def call(self, x, encoder_output, lookahed_mask, padding_mask, training):\n        multihead_output1 = self.multihead_attention1(x, x, x, lookahed_mask)\n        multihead_output1 = self.dropout1(multihead_output1, training=training)\n        addnorm_output1 = self.add_norm1(x, multihead_output1)\n\n        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n        multihead_output2 = self.dropout2(multihead_output2, training=training)\n        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n        \n        feedforward_output = self.feed_forward(addnorm_output2)\n        feedforward_output = self.dropout3(feedforward_output, training=training)\n \n        return self.add_norm3(addnorm_output2, feedforward_output)","metadata":{"cell_id":"6de110b1f6c64b328b2f76ece9a44bb8","source_hash":"84c8aede","execution_start":1684777165851,"execution_millis":5,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Implementing the Decoder","metadata":{"cell_id":"e686836f5a6a497b9197fcaf47b4d0a0","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"class Decoder(Layer):\n    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n        self.dropout = Dropout(rate)\n        self.decoder_layer = [DecoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n\n    def call(self, output_target, encoder_output, lookahed_mask, padding_mask, training):\n        pos_encoding_output = self.pos_encoding(output_target)\n        x = self.dropout(pos_encoding_output, training=training)\n\n        for i, layer in enumerate(self.decoder_layer):\n            x = layer(x, encoder_output, lookahed_mask, padding_mask, training)\n\n        return x","metadata":{"cell_id":"45d19e747e864014b3fff406676e51de","source_hash":"2c71c4fa","execution_start":1684777731695,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"### Test","metadata":{"cell_id":"6cdf6f7032434a1e8e44dfb3e2c7e3c5","formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"from numpy import random\n \ndec_vocab_size = 20  # Vocabulary size for the decoder\ninput_seq_length = 5  # Maximum length of the input sequence\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nn = 6  # Number of layers in the decoder stack\n \nbatch_size = 64  # Batch size from the training process\ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n \ninput_seq = random.random((batch_size, input_seq_length))\nenc_output = random.random((batch_size, input_seq_length, d_model))\n \ndecoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)","metadata":{"cell_id":"e8fd216747eb4da19b44ddd9b6c06636","source_hash":"1da710b","execution_start":1684775772359,"execution_millis":140,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(decoder(input_seq, enc_output, None, True))","metadata":{"cell_id":"0a97546dce044bef88dc98ac6b70b1f6","source_hash":"f9d061fd","execution_start":1684775775808,"execution_millis":1217,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-05-22 17:16:15.751695: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-05-22 17:16:15.751738: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-05-22 17:16:15.751757: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-b5f739dc-f641-4c72-a448-d84edd2bf5bd): /proc/driver/nvidia/version does not exist\n2023-05-22 17:16:15.753005: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\ntf.Tensor(\n[[[-1.45526004e+00  2.79848576e-01 -1.08611488e+00 ... -1.16155267e+00\n    1.97381592e+00 -3.56632352e-01]\n  [-1.34103954e+00  1.94245160e-01 -1.03848279e+00 ... -1.08624864e+00\n    1.99266410e+00 -3.15863580e-01]\n  [-1.35300672e+00  5.54456599e-02 -1.06689787e+00 ... -1.02608144e+00\n    1.99410844e+00 -2.28193626e-01]\n  [-1.45535338e+00  1.00328589e-05 -1.16145051e+00 ... -9.90065217e-01\n    1.98078895e+00 -1.74791574e-01]\n  [-1.56076586e+00  7.87177607e-02 -1.23194027e+00 ... -9.65437710e-01\n    1.96890247e+00 -1.59906343e-01]]\n\n [[-1.18488872e+00  5.67315578e-01 -8.83649409e-01 ... -1.07432067e+00\n    2.31274128e+00  1.47498518e-01]\n  [-1.07827365e+00  4.91131842e-01 -8.45879376e-01 ... -9.90709662e-01\n    2.33177114e+00  1.95945203e-01]\n  [-1.08539045e+00  3.59172881e-01 -8.65060151e-01 ... -9.23437715e-01\n    2.31611919e+00  2.52160937e-01]\n  [-1.20067167e+00  3.19754094e-01 -9.59400892e-01 ... -8.89226973e-01\n    2.28121424e+00  2.73847073e-01]\n  [-1.34492898e+00  4.17923629e-01 -1.02683067e+00 ... -8.71912897e-01\n    2.26610994e+00  2.81994402e-01]]\n\n [[-1.09736669e+00  4.96351749e-01 -1.14288199e+00 ... -1.28561270e+00\n    2.40923738e+00 -1.19208381e-01]\n  [-9.76836383e-01  4.15620834e-01 -1.07824290e+00 ... -1.18328667e+00\n    2.41782880e+00 -6.41829595e-02]\n  [-9.89553511e-01  2.85213023e-01 -1.09830046e+00 ... -1.10009301e+00\n    2.41717100e+00 -2.12210719e-03]\n  [-1.11344457e+00  2.42108524e-01 -1.20079494e+00 ... -1.07413363e+00\n    2.39318490e+00  3.46214436e-02]\n  [-1.23881233e+00  3.42222393e-01 -1.29695868e+00 ... -1.09947407e+00\n    2.37452817e+00  2.89937798e-02]]\n\n ...\n\n [[-1.08935094e+00  3.54838967e-01 -1.02535236e+00 ... -1.12301648e+00\n    1.91219306e+00 -2.17206582e-01]\n  [-9.72129583e-01  2.94358015e-01 -9.59505260e-01 ... -1.04771447e+00\n    1.92681503e+00 -1.40763208e-01]\n  [-9.75782812e-01  1.63334712e-01 -9.91848350e-01 ... -9.79068041e-01\n    1.92538393e+00 -4.84649129e-02]\n  [-1.07807100e+00  1.15560867e-01 -1.09841990e+00 ... -9.34919178e-01\n    1.88702524e+00  1.52088348e-02]\n  [-1.22675681e+00  1.83698282e-01 -1.20274246e+00 ... -9.30711329e-01\n    1.88431656e+00  2.18872260e-02]]\n\n [[-1.34937704e+00  2.00383782e-01 -1.07267511e+00 ... -1.27164507e+00\n    2.00456357e+00 -2.15741768e-01]\n  [-1.23481464e+00  1.35512620e-01 -9.99843001e-01 ... -1.19072497e+00\n    2.00390053e+00 -1.51313171e-01]\n  [-1.25699353e+00 -5.16463071e-03 -1.00866246e+00 ... -1.12694228e+00\n    1.98478436e+00 -7.96457306e-02]\n  [-1.39386976e+00 -5.51248454e-02 -1.10633290e+00 ... -1.08274508e+00\n    1.95009375e+00 -3.94145101e-02]\n  [-1.53359342e+00  3.60014923e-02 -1.18363154e+00 ... -1.07547331e+00\n    1.93780148e+00 -4.92498539e-02]]\n\n [[-1.22446895e+00  4.08633441e-01 -1.00510108e+00 ... -1.27633786e+00\n    2.24831009e+00 -2.88054217e-02]\n  [-1.13900089e+00  3.50916535e-01 -9.38355625e-01 ... -1.17372870e+00\n    2.24301171e+00  1.15280291e-02]\n  [-1.14919698e+00  2.24357188e-01 -9.54761744e-01 ... -1.08995128e+00\n    2.21236396e+00  6.28105849e-02]\n  [-1.26164603e+00  1.88772321e-01 -1.08083642e+00 ... -1.04311132e+00\n    2.16358089e+00  1.01253405e-01]\n  [-1.39020526e+00  2.85840243e-01 -1.17938638e+00 ... -1.03105116e+00\n    2.16074920e+00  1.31298676e-01]]], shape=(64, 5, 512), dtype=float32)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# from encoder import Encoder\n# from decoder import Decoder\nfrom tensorflow import math, cast, float32, linalg, ones, maximum, newaxis\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense","metadata":{"cell_id":"bb86400c29a846a081a866c4203e42bf","source_hash":"c85a9dbd","execution_start":1684775925283,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class TransformerModel(Model):\n    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n        super(TransformerModel, self).__init__(**kwargs)\n\n        #setup the encoder\n        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        #setup decoder\n        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        #final dense layer\n        self.model_last_layer = Dense(dec_vocab_size)\n\n    def padding_mask(self, input):\n        mask = math.equal(input, 0)\n        mask = cast(mask, float32)\n\n        return mask[:, newaxis, newaxis, :]\n\n    def lookahed_mask(self, shape):\n        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n        return mask\n\n    def call(self, encoder_input, decoder_input, training):\n        enc_padding_mask = self.padding_mask(encoder_input)\n\n        dec_in_padding_mask = self.padding_mask(decoder_input)\n        dec_in_lookahead_mask = self.lookahed_mask(decoder_input.shape[1])\n        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n\n        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n\n        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n\n        model_output = self.model_last_layer(decoder_output)\n\n        return model_output        ","metadata":{"cell_id":"4bc7d8a126974665963d19c483dcf937","source_hash":"5a4c5a52","execution_start":1684776992924,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":18},{"cell_type":"code","source":"h = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nn = 6  # Number of layers in the encoder stack\n \ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers","metadata":{"cell_id":"d2da2949937a47e89843ca707dd8d8d0","source_hash":"5f256eaf","execution_start":1684776906827,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"enc_vocab_size = 20 # Vocabulary size for the encoder\ndec_vocab_size = 20 # Vocabulary size for the decoder\n \nenc_seq_length = 5  # Maximum length of the input sequence\ndec_seq_length = 5  # Maximum length of the target sequence","metadata":{"cell_id":"24a22bd3cf1547279072168b36a1a47f","source_hash":"4149a81f","execution_start":1684776927452,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# from model import TransformerModel\n \ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)","metadata":{"cell_id":"143ffaec4a8944c5886b8de6c23c2c11","source_hash":"e3825f48","execution_start":1684776996836,"execution_millis":206,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":19},{"cell_type":"code","source":"encoder = EncoderLayer(enc_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\nencoder.build_graph().summary()\n \ndecoder = DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\ndecoder.build_graph().summary()","metadata":{"cell_id":"b5d04ecd853c4ff7b371126213dde6e9","source_hash":"16444627","execution_start":1684777813799,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"__init__() takes 7 positional arguments but 8 were given","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m encoder\u001b[38;5;241m.\u001b[39mbuild_graph()\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m      4\u001b[0m decoder \u001b[38;5;241m=\u001b[39m DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n","\u001b[0;31mTypeError\u001b[0m: __init__() takes 7 positional arguments but 8 were given"]}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"cell_id":"c2fd2c9e698b4662b091189dfd94591a","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b5f739dc-f641-4c72-a448-d84edd2bf5bd' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"cedf44b54e764d83a0e083ae17d74a44","deepnote_execution_queue":[]}}